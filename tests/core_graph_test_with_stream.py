"""
Coreæ¨¡å—æµå¼è¾“å‡ºæµ‹è¯•
å±•ç¤ºï¼šå¤šè½®å¯¹è¯ï¼ˆæµå¼ï¼‰ã€æƒé™ç®¡ç†ï¼ˆæµå¼ï¼‰ã€åˆ†æ”¯å¯¹è¯ï¼ˆæµå¼ï¼‰
"""

import asyncio
from typing import Dict, Any
from datetime import datetime

# æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ å·²ç»å°†æ”¹é€ åçš„æ–‡ä»¶æ”¾åˆ°äº†æ­£ç¡®ä½ç½®
# å¦‚æœä½¿ç”¨æ–°æ–‡ä»¶åï¼Œè¯·è°ƒæ•´å¯¼å…¥è·¯å¾„
from core.graph import create_multi_agent_graph
from core.controller import ExecutionController, ControllerEventType
from utils.logger import get_logger, set_global_debug

logger = get_logger("ArtifactFlow")
set_global_debug(False)


# ============================================================
# æµå¼äº‹ä»¶å¤„ç†å™¨
# ============================================================

class StreamEventHandler:
    """æµå¼äº‹ä»¶å¤„ç†å™¨ - ç¾åŒ–è¾“å‡º"""
    
    def __init__(self, verbose: bool = True):
        """
        åˆå§‹åŒ–äº‹ä»¶å¤„ç†å™¨
        
        Args:
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆTokenä½¿ç”¨ã€å·¥å…·å‚æ•°ç­‰ï¼‰
        """
        self.verbose = verbose
        self.current_agent = None
        self.llm_buffer = ""
        self.reasoning_buffer = ""  # ğŸ†• ç”¨äºç¼“å†² reasoning_content
        self.start_time = None
    
    def handle_metadata(self, data: Dict):
        """å¤„ç†å…ƒæ•°æ®äº‹ä»¶"""
        self.start_time = datetime.now()
        print("\n" + "-"*80)
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ")
        if data.get("resuming"):
            print(f"çŠ¶æ€: ğŸ”„ ä»ä¸­æ–­æ¢å¤")
        print("-"*80)
    
    def handle_stream(self, data: Dict):
        """å¤„ç†æµå¼å†…å®¹äº‹ä»¶"""
        stream_type = data["type"]
        agent = data["agent"]
        event_data = data.get("data")
        
        # Agent åˆ‡æ¢
        if agent != self.current_agent:
            if self.current_agent and (self.llm_buffer or self.reasoning_buffer):
                print()  # æ¢è¡Œ
            self.current_agent = agent
            self.llm_buffer = ""
            self.reasoning_buffer = ""
        
        if stream_type == "start":
            print(f"\n[{agent}] â° å¼€å§‹æ‰§è¡Œ...")
        
        elif stream_type == "llm_chunk":
            if event_data:
                # ğŸ†• å¤„ç† reasoning_contentï¼ˆæ€è€ƒå†…å®¹ï¼‰
                reasoning = event_data.get("reasoning_content")
                if reasoning:
                    # åªæ˜¾ç¤ºæ–°å¢çš„æ€è€ƒå†…å®¹
                    if reasoning.startswith(self.reasoning_buffer):
                        new_reasoning = reasoning[len(self.reasoning_buffer):]
                        if new_reasoning:
                            # ç¬¬ä¸€æ¬¡æ˜¾ç¤ºæ€è€ƒæ—¶ï¼Œæ·»åŠ æ ‡è®°
                            if not self.reasoning_buffer:
                                print(f"\n[{agent}] ğŸ’­ æ€è€ƒä¸­...", flush=True)
                            print(f"\033[90m{new_reasoning}\033[0m", end="", flush=True)  # ç°è‰²æ˜¾ç¤º
                            self.reasoning_buffer = reasoning
                    else:
                        # æ€è€ƒå†…å®¹é‡ç½®ï¼ˆæ–°ä¸€è½®ï¼‰
                        if self.reasoning_buffer:
                            print()  # æ¢è¡Œ
                        print(f"\n[{agent}] ğŸ’­ æ€è€ƒä¸­...", flush=True)
                        print(f"\033[90m{reasoning}\033[0m", end="", flush=True)
                        self.reasoning_buffer = reasoning
                
                # å¤„ç† contentï¼ˆæ­£å¸¸è¾“å‡ºï¼‰
                content = event_data.get("content")
                if content:
                    # å¦‚æœæœ‰æ€è€ƒå†…å®¹ï¼Œå…ˆæ¢è¡Œå†æ˜¾ç¤ºæ­£å¸¸å†…å®¹
                    if self.reasoning_buffer and not self.llm_buffer:
                        print(f"\n[{agent}] ğŸ’¬ å›ç­”:", flush=True)
                    
                    # åªæ˜¾ç¤ºæ–°å¢å†…å®¹
                    if content.startswith(self.llm_buffer):
                        new_content = content[len(self.llm_buffer):]
                        print(new_content, end="", flush=True)
                        self.llm_buffer = content
                    else:
                        # å†…å®¹é‡ç½®ï¼ˆæ–°ä¸€è½®ï¼‰
                        if self.llm_buffer:
                            print()  # æ¢è¡Œ
                        print(content, end="", flush=True)
                        self.llm_buffer = content

        elif stream_type == "llm_complete":
            if self.llm_buffer or self.reasoning_buffer:
                print()  # æ¢è¡Œ
            print(f"[{agent}] âœ… LLM è¾“å‡ºå®Œæˆ")
            self.llm_buffer = ""
            self.reasoning_buffer = ""
            
            if self.verbose and event_data:
                token_usage = event_data.get("token_usage", {})
                if token_usage:
                    input_tokens = token_usage.get("input_tokens", 0)
                    output_tokens = token_usage.get("output_tokens", 0)
                    print(f"[{agent}] ğŸ“Š Token: {input_tokens} in / {output_tokens} out")
        
        elif stream_type == "tool_start":
            if self.llm_buffer or self.reasoning_buffer:
                print()  # æ¢è¡Œ
            print(f"[{agent}] ğŸ”§ è°ƒç”¨å·¥å…·...")
            self.llm_buffer = ""
            self.reasoning_buffer = ""
        
        elif stream_type == "tool_result":
            print(f"[{agent}] âœ… å·¥å…·è°ƒç”¨å®Œæˆ")
            
            if self.verbose and event_data:
                tool_calls = event_data.get("tool_calls", [])
                if tool_calls:
                    last_call = tool_calls[-1]
                    print(f"[{agent}]    å·¥å…·: {last_call['tool']}")
                    success = last_call['result']['success']
                    status = "âœ“" if success else "âœ—"
                    print(f"[{agent}]    ç»“æœ: {status}")
        
        elif stream_type == "permission_required":
            if self.llm_buffer or self.reasoning_buffer:
                print()
            print(f"\n[{agent}] âš ï¸ éœ€è¦æƒé™ç¡®è®¤")
            if event_data and event_data.get("routing"):
                routing = event_data["routing"]
                print(f"[{agent}]    å·¥å…·: {routing['tool_name']}")
                print(f"[{agent}]    æƒé™çº§åˆ«: {routing['permission_level']}")
            self.llm_buffer = ""
            self.reasoning_buffer = ""
        
        elif stream_type == "complete":
            if self.llm_buffer or self.reasoning_buffer:
                print()
            print(f"[{agent}] ğŸ‰ æ‰§è¡Œå®Œæˆ")
            self.llm_buffer = ""
            self.reasoning_buffer = ""
        
        elif stream_type == "error":
            if self.llm_buffer or self.reasoning_buffer:
                print()
            print(f"\n[{agent}] âŒ æ‰§è¡Œé”™è¯¯")
            if event_data:
                print(f"[{agent}]    é”™è¯¯: {event_data.get('content')}")
            self.llm_buffer = ""
            self.reasoning_buffer = ""
    
    def handle_complete(self, data: Dict):
        """å¤„ç†å®Œæˆäº‹ä»¶"""
        if self.llm_buffer or self.reasoning_buffer:
            print()  # ç¡®ä¿æ¢è¡Œ
        
        elapsed = (datetime.now() - self.start_time).total_seconds() if self.start_time else 0
        
        print("\n" + "-"*80)
        if data["success"]:
            if data.get("interrupted"):
                print(f"âš ï¸  æ‰§è¡Œä¸­æ–­")
                print(f"   ä¸­æ–­ç±»å‹: {data['interrupt_type']}")
                print(f"   è€—æ—¶: {elapsed:.2f}s")
            else:
                print(f"âœ… æ‰§è¡ŒæˆåŠŸå®Œæˆ")
                print(f"   è€—æ—¶: {elapsed:.2f}s")
                if not self.verbose and data.get("response"):
                    response = data["response"]
                    preview = response[:150] + "..." if len(response) > 150 else response
                    print(f"   å“åº”: {preview}")
        else:
            print(f"âŒ æ‰§è¡Œå¤±è´¥")
            print(f"   é”™è¯¯: {data.get('error')}")
            print(f"   è€—æ—¶: {elapsed:.2f}s")
        print("-"*80 + "\n")
        
        self.llm_buffer = ""
        self.reasoning_buffer = ""
    
    async def process_stream(self, stream_generator):
        """å¤„ç†æ•´ä¸ªæµå¼è¿‡ç¨‹"""
        result_data = None
        
        async for event in stream_generator:
            event_type = event["event_type"]
            data = event["data"]
            
            if event_type == ControllerEventType.METADATA:
                self.handle_metadata(data)
            elif event_type == ControllerEventType.STREAM:
                self.handle_stream(data)
            elif event_type == ControllerEventType.COMPLETE:
                self.handle_complete(data)
                result_data = data
        
        return result_data


# ============================================================
# æµ‹è¯•åœºæ™¯
# ============================================================

async def demo_multi_turn_conversation():
    """æ¼”ç¤ºå¤šè½®å¯¹è¯ï¼ˆæµå¼ï¼‰"""
    logger.debug("="*60)
    logger.debug("ğŸ“ å¤šè½®å¯¹è¯æ¼”ç¤ºï¼ˆæµå¼ï¼‰")
    logger.debug("="*60)
    
    # åˆ›å»ºç³»ç»Ÿ
    compiled_graph = create_multi_agent_graph()
    controller = ExecutionController(compiled_graph)
    handler = StreamEventHandler(verbose=True)
    
    # ç¬¬ä¸€è½®
    print("\n" + "ğŸ—£ï¸  ç”¨æˆ·: ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ")
    result1 = await handler.process_stream(
        controller.stream_execute(content="ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ")
    )
    conv_id = result1["conversation_id"]
    
    # ç­‰å¾…ä¸€ä¸‹ï¼Œè®©ç”¨æˆ·çœ‹æ¸…è¾“å‡º
    await asyncio.sleep(1)
    
    # ç¬¬äºŒè½®ï¼ˆæœ‰å¯¹è¯å†å²ï¼‰
    print("\n" + "ğŸ—£ï¸  ç”¨æˆ·: å¸®æˆ‘æ•´ç†åˆ°artifactä¸­ï¼Œå†…å®¹æµ…æ˜¾æ˜“æ‡‚ä¸€ç‚¹")
    result2 = await handler.process_stream(
        controller.stream_execute(
            content="å¸®æˆ‘æ•´ç†åˆ°artifactä¸­ï¼Œå†…å®¹æµ…æ˜¾æ˜“æ‡‚ä¸€ç‚¹",
            conversation_id=conv_id
        )
    )
    
    await asyncio.sleep(1)
    
    # ç¬¬ä¸‰è½®
    print("\n" + "ğŸ—£ï¸  ç”¨æˆ·: å¸®æˆ‘å†™ä¸€ä»½æœ€æ–°çš„ç ”ç©¶è¿›å±•æŠ¥å‘Š")
    result3 = await handler.process_stream(
        controller.stream_execute(
            content="å¸®æˆ‘å†™ä¸€ä»½æœ€æ–°çš„ç ”ç©¶è¿›å±•æŠ¥å‘Š",
            conversation_id=conv_id
        )
    )
    
    print("\nâœ¨ å¤šè½®å¯¹è¯æ¼”ç¤ºå®Œæˆï¼")


async def demo_permission_flow():
    """æ¼”ç¤ºæƒé™ç¡®è®¤æµç¨‹ï¼ˆæµå¼ï¼‰"""
    logger.debug("="*60)
    logger.debug("ğŸ” æƒé™ç¡®è®¤æ¼”ç¤ºï¼ˆæµå¼ï¼‰")
    logger.debug("="*60)
    
    # é…ç½®æƒé™
    from tools.base import ToolPermission
    tool_permissions = {
        "web_fetch": ToolPermission.CONFIRM
    }
    
    compiled_graph = create_multi_agent_graph(tool_permissions=tool_permissions)
    controller = ExecutionController(compiled_graph)
    handler = StreamEventHandler(verbose=True)
    
    # å‘èµ·éœ€è¦çˆ¬è™«çš„ä»»åŠ¡
    print("\n" + "ğŸ—£ï¸  ç”¨æˆ·: è¯·æŠ“å–å¹¶åˆ†æ https://github.com/langchain-ai/langgraph çš„å†…å®¹")
    result = await handler.process_stream(
        controller.stream_execute(
            content="è¯·æŠ“å–å¹¶åˆ†æ https://github.com/langchain-ai/langgraph çš„å†…å®¹"
        )
    )
    
    # âœ… å¾ªç¯å¤„ç†å¤šæ¬¡ä¸­æ–­
    max_retries = 3  # æœ€å¤šå¤„ç†3æ¬¡æƒé™ç¡®è®¤
    retry_count = 0
    
    while result.get("interrupted") and retry_count < max_retries:
        retry_count += 1
        print(f"\nğŸ’­ ç³»ç»Ÿè¯·æ±‚æƒé™ç¡®è®¤... (ç¬¬ {retry_count} æ¬¡)")
        print(f"   å·¥å…·: {result['interrupt_data']['tool_name']}")
        print(f"   å‚æ•°: {result['interrupt_data']['params']}")
        
        # æ¨¡æ‹Ÿç”¨æˆ·å†³ç­–
        print("\nğŸ¤” ç”¨æˆ·æ€è€ƒä¸­...")
        await asyncio.sleep(2)
        
        approved = False
        
        if approved:
            print("\nâœ… ç”¨æˆ·æ‰¹å‡†ï¼Œç»§ç»­æ‰§è¡Œ...")
        else:
            print("\nâŒ ç”¨æˆ·æ‹’ç»ï¼Œå°è¯•å…¶ä»–æ–¹å¼...")
        
        # ç»§ç»­æ‰§è¡Œ
        result = await handler.process_stream(
            controller.stream_execute(
                thread_id=result["thread_id"],
                resume_data={"type": "permission", "approved": approved}
            )
        )
    
    if retry_count >= max_retries:
        print(f"\nâš ï¸ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
    
    print("\nâœ¨ æƒé™ç¡®è®¤æ¼”ç¤ºå®Œæˆï¼")


async def demo_branch_conversation():
    """æ¼”ç¤ºåˆ†æ”¯å¯¹è¯ï¼ˆæµå¼ï¼‰"""
    logger.debug("="*60)
    logger.debug("ğŸŒ¿ åˆ†æ”¯å¯¹è¯æ¼”ç¤ºï¼ˆæµå¼ï¼‰")
    logger.debug("="*60)
    
    compiled_graph = create_multi_agent_graph()
    controller = ExecutionController(compiled_graph)
    handler = StreamEventHandler(verbose=False)  # ç®€åŒ–è¾“å‡º
    
    # ä¸»çº¿å¯¹è¯
    print("\n" + "ğŸ—£ï¸  ç”¨æˆ·: è®¡ç®— 15 + 28 ç­‰äºå¤šå°‘")
    result1 = await handler.process_stream(
        controller.stream_execute(content="è®¡ç®— 15 + 28 ç­‰äºå¤šå°‘")
    )
    conv_id = result1["conversation_id"]
    msg1_id = result1["message_id"]
    
    await asyncio.sleep(1)
    
    # ç»§ç»­ä¸»çº¿
    print("\n" + "ğŸ—£ï¸  ç”¨æˆ·: å†ä¹˜ä»¥2")
    result2 = await handler.process_stream(
        controller.stream_execute(
            content="å†ä¹˜ä»¥2",
            conversation_id=conv_id
        )
    )
    
    await asyncio.sleep(1)
    
    # ä»msg1åˆ›å»ºåˆ†æ”¯
    print("\n" + "ğŸŒ¿ ä»ç¬¬ä¸€æ¡æ¶ˆæ¯åˆ›å»ºåˆ†æ”¯...")
    print("ğŸ—£ï¸  ç”¨æˆ·: å†å‡å»ä¸€ä¸‡")
    result3 = await handler.process_stream(
        controller.stream_execute(
            content="å†å‡å»ä¸€ä¸‡",
            conversation_id=conv_id,
            parent_message_id=msg1_id  # ä»msg1åˆ†æ”¯
        )
    )
    
    print("ğŸ“Š å¯¹è¯æ ‘ç»“æ„:")
    print("   msg1: '15 + 28 = ?'")
    print("   â”œâ”€ msg2: 'å†ä¹˜ä»¥2'  â† ä¸»çº¿")
    print("   â””â”€ msg3: 'å†å‡å»ä¸€ä¸‡' â† åˆ†æ”¯")
    
    print("\nâœ¨ åˆ†æ”¯å¯¹è¯æ¼”ç¤ºå®Œæˆï¼")


async def demo_compare_batch_vs_stream():
    """å¯¹æ¯”æ‰¹é‡æ¨¡å¼ vs æµå¼æ¨¡å¼"""
    logger.debug("="*60)
    logger.debug("âš–ï¸  æ‰¹é‡ vs æµå¼å¯¹æ¯”")
    logger.debug("="*60)
    
    compiled_graph = create_multi_agent_graph()
    controller = ExecutionController(compiled_graph)
    
    question = "ç®€å•ä»‹ç»ä¸€ä¸‹ Python"
    
    # æ‰¹é‡æ¨¡å¼
    print("\n" + "="*80)
    print("ğŸ“¦ æ‰¹é‡æ¨¡å¼")
    print("="*80)
    print(f"\nğŸ—£ï¸  ç”¨æˆ·: {question}")
    print("\nâ³ ç­‰å¾…ä¸­...")
    
    start_time = datetime.now()
    result = await controller.execute(content=question)
    elapsed = (datetime.now() - start_time).total_seconds()
    
    if result["success"]:
        print(f"\nâœ… æ”¶åˆ°å®Œæ•´å“åº” (è€—æ—¶ {elapsed:.2f}s):")
        print(f"\n{result['response'][:200]}...")
    
    await asyncio.sleep(2)
    
    # æµå¼æ¨¡å¼
    print("\n" + "="*80)
    print("âš¡ æµå¼æ¨¡å¼")
    print("="*80)
    print(f"\nğŸ—£ï¸  ç”¨æˆ·: {question}")
    print("\nğŸ’¬ å®æ—¶è¾“å‡º:\n")
    
    handler = StreamEventHandler(verbose=False)
    start_time = datetime.now()
    result = await handler.process_stream(
        controller.stream_execute(content=question)
    )
    elapsed = (datetime.now() - start_time).total_seconds()
    
    print("âœ¨ å¯¹æ¯”æ¼”ç¤ºå®Œæˆï¼")


# ============================================================
# ä¸»ç¨‹åº
# ============================================================

async def main():
    print("\n" + "ğŸ¤– ArtifactFlow æµå¼è¾“å‡ºæµ‹è¯•")
    
    # é€‰æ‹©æ¼”ç¤º
    demos = {
        "1": ("å¤šè½®å¯¹è¯ï¼ˆæµå¼ï¼‰", demo_multi_turn_conversation),
        "2": ("æƒé™ç¡®è®¤ï¼ˆæµå¼ï¼‰", demo_permission_flow),
        "3": ("åˆ†æ”¯å¯¹è¯ï¼ˆæµå¼ï¼‰", demo_branch_conversation),
        "4": ("æ‰¹é‡ vs æµå¼å¯¹æ¯”", demo_compare_batch_vs_stream),
        "5": ("å…¨éƒ¨æ¼”ç¤º", None)
    }
    
    print("\n" + "é€‰æ‹©æ¼”ç¤º:")
    print()
    for key, (name, _) in demos.items():
        print(f"  {key}. {name}")
    
    choice = input("\nğŸ‘‰ é€‰æ‹© (1-7): ").strip()
    
    try:
        if choice == "7":
            # å…¨éƒ¨æ¼”ç¤º
            for key in ["1", "2", "3", "4", "5", "6"]:
                await demos[key][1]()
                print("\n" + "-"*80)
                await asyncio.sleep(2)
        elif choice in demos:
            await demos[choice][1]()
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*80)
    print("ğŸ‘‹ æµ‹è¯•ç»“æŸ")
    print("="*80 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
"""
ç»Ÿä¸€çš„LLMæ¥å£å°è£…
åŸºäºLangChainå®ç°ï¼Œæ”¯æŒå¤šç§æ¨¡å‹æä¾›å•†
"""

import os
from typing import Optional, Dict, Any, Union
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI

# å°è¯•å¯¼å…¥ç¤¾åŒºæ¨¡å‹ï¼ˆå¯é€‰ä¾èµ–ï¼‰
try:
    from langchain_community.chat_models.tongyi import ChatTongyi
    TONGYI_AVAILABLE = True
except ImportError:
    TONGYI_AVAILABLE = False
    ChatTongyi = None

try:
    from langchain_deepseek import ChatDeepSeek
    DEEPSEEK_AVAILABLE = True
except ImportError:
    DEEPSEEK_AVAILABLE = False
    ChatDeepSeek = None


from utils.logger import get_logger

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

logger = get_logger("Models")


# é¢„å®šä¹‰æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    # OpenAI
    "gpt-4o": {
        "provider": "openai",
        "model": "gpt-4o",
        "api_key": os.getenv("OPENAI_API_KEY"),
    },
    "gpt-4o-mini": {
        "provider": "openai",
        "model": "gpt-4o-mini", 
        "api_key": os.getenv("OPENAI_API_KEY"),
    },
    
    # Qwen (é€šä¹‰åƒé—®) - ä½¿ç”¨ChatTongyi
    "qwen-flash": {
        "provider": "dashscope",
        "model": "qwen-flash",
        "api_key": os.getenv("DASHSCOPE_API_KEY"),
    },
    "qwen-plus": {
        "provider": "dashscope",
        "model": "qwen-plus",
        "api_key": os.getenv("DASHSCOPE_API_KEY"),
    },
    
    # Qwen3-30B ç³»åˆ—æ¨¡å‹
    "qwen3-30b-thinking": {
        "provider": "dashscope",  # ChatTongyiåº”è¯¥èƒ½å¤„ç†thinkingæ¨¡å‹
        "model": "qwen3-30b-a3b-thinking-2507",
        "api_key": os.getenv("DASHSCOPE_API_KEY"),
        "support_reasoning": True,
        "description": "Qwen3-30Bæ€è€ƒæ¨¡å‹ï¼Œæ”¯æŒæ·±åº¦æ¨ç†"
    },
    "qwen3-30b-instruct": {
        "provider": "dashscope",
        "model": "qwen3-30b-a3b-instruct-2507",
        "api_key": os.getenv("DASHSCOPE_API_KEY"),
        "description": "Qwen3-30BæŒ‡ä»¤æ¨¡å‹ï¼Œå¿«é€Ÿå“åº”"
    },
    
    # DeepSeek
    "deepseek-chat": {
        "provider": "deepseek",
        "model": "deepseek-chat",
        "api_key": os.getenv("DEEPSEEK_API_KEY"),
    },
    "deepseek-reasoner": {
        "provider": "deepseek",
        "model": "deepseek-reasoner",
        "api_key": os.getenv("DEEPSEEK_API_KEY"),
        "support_reasoning": True,
        "description": "DeepSeekæ¨ç†æ¨¡å‹"
    },
}


def create_llm(
    model: str = "gpt-4o-mini",
    temperature: float = 0.7,
    max_tokens: int = 4096,
    streaming: bool = False,
    **kwargs
):
    """
    åˆ›å»ºLLMå®ä¾‹
    
    Args:
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        streaming: æ˜¯å¦æµå¼è¾“å‡º
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        ChatModelå®ä¾‹ (ChatOpenAI/ChatTongyi/ChatDeepSeek)
    
    Example:
        # ä½¿ç”¨é¢„å®šä¹‰æ¨¡å‹
        llm = create_llm("qwen-plus")
        response = llm.invoke("Hello!")
        print(response.content)  # æ ‡å‡†å›ç­”
        
        # ä½¿ç”¨æ€è€ƒæ¨¡å‹
        llm = create_llm("qwen3-30b-thinking")
        response = llm.invoke("è§£é‡Šé‡å­çº ç¼ ")
        # å¦‚æœChatTongyiæ”¯æŒï¼Œreasoning_contentä¼šåœ¨responseçš„å±æ€§ä¸­
        if hasattr(response, 'reasoning_content'):
            print(response.reasoning_content)  # æ€è€ƒè¿‡ç¨‹
        print(response.content)  # æœ€ç»ˆç­”æ¡ˆ
    """
    
    # è·å–é…ç½®
    if model in MODEL_CONFIGS:
        config = MODEL_CONFIGS[model].copy()
        provider = config.pop("provider")
        model_name = config.pop("model")
        support_reasoning = config.pop("support_reasoning", False)
        config.pop("description", None)
        
        # åˆå¹¶ç”¨æˆ·å‚æ•°
        config.update(kwargs)
        
        # æ ¹æ®provideråˆ›å»ºå¯¹åº”çš„æ¨¡å‹
        if provider == "openai":
            llm = ChatOpenAI(
                model=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                streaming=streaming,
                **config
            )
            logger.info(f"Created ChatOpenAI: {model_name}")
        
        elif provider == "dashscope" and TONGYI_AVAILABLE:
                llm = ChatTongyi(
                    model=model_name,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    streaming=streaming,
                    **config
                )
                logger.info(f"Created ChatTongyi: {model_name}")
 
        elif provider == "deepseek" and DEEPSEEK_AVAILABLE:
            llm = ChatDeepSeek(
                model=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                streaming=streaming,
                **config
            )
            logger.info(f"Created ChatDeepSeek: {model_name}")
        
        else:
            # é™çº§åˆ°ChatOpenAIï¼ˆé€šç”¨OpenAIå…¼å®¹æ¥å£ï¼‰
            logger.warning(f"Provider {provider} not available, using ChatOpenAI fallback")
            
            # è®¾ç½®base_url
            if provider == "tongyi":
                config["base_url"] = "https://dashscope.aliyuncs.com/compatible-mode/v1"
            elif provider == "deepseek":
                config["base_url"] = "https://api.deepseek.com/v1"
            
            llm = ChatOpenAI(
                model=model_name,
                temperature=temperature,
                max_tokens=max_tokens,
                streaming=streaming,
                **config
            )
            logger.info(f"Created ChatOpenAI (fallback): {model_name}")
        
        return llm
    
    else:
        # ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹ï¼Œä½¿ç”¨ChatOpenAI
        llm = ChatOpenAI(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            streaming=streaming,
            **kwargs
        )
        logger.info(f"Created custom LLM: {model}")
        return llm


def get_available_models() -> list[str]:
    """è·å–æ‰€æœ‰é¢„å®šä¹‰çš„æ¨¡å‹åç§°"""
    return list(MODEL_CONFIGS.keys())


def get_model_info(model: str) -> Dict[str, Any]:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    if model in MODEL_CONFIGS:
        config = MODEL_CONFIGS[model].copy()
        return {
            "model_id": config["model"],
            "provider": config["provider"],
            "description": config.get("description", ""),
        }
    return {"model_id": model, "provider": "unknown", "description": ""}


if __name__ == "__main__":
    # æ£€æŸ¥å¯ç”¨çš„æä¾›å•†
    print("\nğŸ“¦ æ£€æŸ¥å¯ç”¨çš„æä¾›å•†:")
    print(f"  - ChatTongyi: {'âœ…' if TONGYI_AVAILABLE else 'âŒ (éœ€è¦å®‰è£… dashscope å’Œ langchain-community)'}")
    print(f"  - ChatDeepSeek: {'âœ…' if DEEPSEEK_AVAILABLE else 'âŒ (éœ€è¦å®‰è£… langchain-deepseek)'}")
    
    # æµ‹è¯•é—®é¢˜
    test_question = "ä¸€ä¸ªåœ†çš„åŠå¾„æ˜¯5ï¼Œå¦ä¸€ä¸ªåœ†çš„åŠå¾„æ˜¯3ï¼Œå¦‚æœè¿™ä¸¤ä¸ªåœ†å¤–åˆ‡ï¼Œæ±‚å®ƒä»¬åœ†å¿ƒä¹‹é—´çš„è·ç¦»ã€‚"
    
    # æµ‹è¯•æ¨¡å‹
    test_models = ["qwen3-30b-thinking", "qwen3-30b-instruct", "deepseek-chat", "deepseek-reasoner"]
    
    for model_name in test_models:
        print("=" * 60)
        try:
            # åˆ›å»ºæ¨¡å‹å¹¶æµ‹è¯•
            llm = create_llm(model_name, temperature=0.3)
            response = llm.invoke(test_question)
            
            print(f"ğŸ“ é—®é¢˜: {test_question}")
            print("-"*60)
            if 'reasoning_content' in response.additional_kwargs:
                print("ğŸ’­ æ€è€ƒ:", response.additional_kwargs.get('reasoning_content', ''))
                print("-"*60)
            print(f"ğŸ’¬ å›ç­”: {response.content}")
            print(response)
        except Exception as e:
            print(f"âŒ è°ƒç”¨å¤±è´¥: {str(e)}")
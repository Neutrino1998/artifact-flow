"""
Crawl Agentå®ç°
è´Ÿè´£ç½‘é¡µå†…å®¹æŠ“å–å’Œä¿¡æ¯æå–
"""

from typing import Dict, Any, Optional, List
from agents.base import BaseAgent, AgentConfig
from utils.logger import get_logger

logger = get_logger("Agents")


class CrawlAgent(BaseAgent):
    """
    Crawl Agent - å†…å®¹æŠ“å–ä¸“å®¶
    
    æ ¸å¿ƒèƒ½åŠ›ï¼š
    1. æ·±åº¦å†…å®¹æŠ“å–ï¼šä»æŒ‡å®šURLæå–è¯¦ç»†ä¿¡æ¯
    2. æ™ºèƒ½å†…å®¹æ¸…æ´—ï¼šå»é™¤æ— å…³å†…å®¹ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯
    3. è´¨é‡åˆ¤æ–­ï¼šè¯†åˆ«åçˆ¬ã€æ— æ•ˆå†…å®¹ç­‰æƒ…å†µ
    4. ç®€æ´è¾“å‡ºï¼šåªè¿”å›æœ‰ä»·å€¼çš„ä¿¡æ¯
    
    å·¥å…·é…ç½®ï¼š
    - web_fetch: ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·
    """
    
    def __init__(self, config: Optional[AgentConfig] = None, toolkit=None):
        """
        åˆå§‹åŒ–Crawl Agent
        
        Args:
            config: Agenté…ç½®
            toolkit: å·¥å…·åŒ…ï¼ˆåº”åŒ…å«web_fetchå·¥å…·ï¼‰
        """
        if not config:
            config = AgentConfig(
                name="crawl_agent",
                description="Web content extraction and cleaning specialist",
                model="qwen-flash",  # å¯ä»¥æ¢æˆæ›´ä¾¿å®œçš„æ¨¡å‹
                temperature=0.3,  # æ›´ä½æ¸©åº¦forç²¾ç¡®æå–
                max_tool_rounds=2,  # é€šå¸¸1-2è½®å³å¯
                streaming=True
            )
        
        super().__init__(config, toolkit)
    
    def build_system_prompt(self, context: Optional[Dict[str, Any]] = None) -> str:
        """
        æ„å»ºCrawl Agentçš„ç³»ç»Ÿæç¤ºè¯
        
        Args:
            context: åŒ…å«URLåˆ—è¡¨å’Œæå–è¦æ±‚çš„ä¸Šä¸‹æ–‡
            
        Returns:
            ç³»ç»Ÿæç¤ºè¯
        """
        prompt = f"""You are {self.config.name}, a specialized agent for web content extraction and cleaning.

## Your Mission

Extract and clean valuable information from web pages.

## Team Context

You are part of a multi-agent research team. The Lead Agent coordinates overall strategy while you focus on deep content extraction."""

        # ğŸŒŸ æ–°å¢ï¼šå¦‚æœæœ‰task_planï¼Œæ·»åŠ å›¢é˜Ÿç›®æ ‡
        if context and context.get("task_plan_content"):
            prompt += f"""

## Team Task Plan

The following is our team's current task plan. Use this to understand what information is most valuable to extract:

{context['task_plan_content']}

**Your Role**: Extract detailed content that supports this plan, focusing on relevant sections."""

        prompt += """

## Core Capabilities

1. **Content Extraction**: Fetch and identify main content
2. **Content Cleaning**: Remove ads, navigation, and irrelevant sections
3. **Quality Assessment**: Detect anti-crawling, paywalls, or invalid content
4. **Concise Output**: Return only valuable information

## Extraction Process

1. Fetch content from URLs
2. Assess content quality
3. Clean and extract key information
4. Format results

## Output Format

Return extracted content in this simple XML structure:

```xml
<extracted_pages>
  <page>
    <url>https://...</url>
    <title>Page Title</title>
    <content>Cleaned and extracted main content</content>
  </page>
  <!-- More pages if needed -->
</extracted_pages>
```

## Important Notes

- If content seems invalid (anti-crawling, paywall, error page), mention it in content field
- Focus on main content, skip navigation/ads/footers
- Keep content concise but informative
- Don't force extraction from clearly invalid pages

## Tool Usage

You have access to the web_fetch tool with these parameters:
- urls: Single URL or list of URLs (required)
- max_content_length: Maximum content per page (default 5000)
- max_concurrent: Concurrent fetches (default 3, max 5)"""
        
        return prompt
    
    def format_final_response(self, content: str, tool_history: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–Crawl Agentçš„æœ€ç»ˆå“åº”
        
        Crawl Agentè‡ªå·±è´Ÿè´£æ¸…æ´—å’Œæ•´ç†ï¼Œç›´æ¥è¿”å›å…¶è¾“å‡º
        """
        return content
    
    async def extract_from_urls(
        self,
        urls: List[str],
        focus_areas: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ä»URLåˆ—è¡¨æå–å†…å®¹
        
        Args:
            urls: è¦æŠ“å–çš„URLåˆ—è¡¨
            focus_areas: å…³æ³¨çš„å†…å®¹é¢†åŸŸ
            
        Returns:
            æå–ç»“æœå­—å…¸
        """
        context = {
            "urls": urls,
            "focus_areas": focus_areas or []
        }
        
        instruction = f"Please extract and clean content from the following {len(urls)} URL(s)."
        if focus_areas:
            instruction += f" Focus on: {', '.join(focus_areas)}"
        instruction += " Return cleaned content in the XML format."
        
        response = await self.execute(instruction, context)
        
        return {
            "success": True,
            "extracted_content": response.content,
            "tool_calls": response.tool_calls,
            "pages_processed": len([c for c in response.tool_calls 
                                   if c.get("tool") == "web_fetch"])
        }


# å·¥å‚å‡½æ•°
def create_crawl_agent(toolkit=None) -> CrawlAgent:
    """
    åˆ›å»ºCrawl Agentå®ä¾‹
    
    Args:
        toolkit: åŒ…å«web_fetchå·¥å…·çš„å·¥å…·åŒ…
        
    Returns:
        é…ç½®å¥½çš„Crawl Agentå®ä¾‹
    """
    return CrawlAgent(toolkit=toolkit)
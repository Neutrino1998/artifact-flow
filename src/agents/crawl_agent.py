"""
Crawl Agentå®ç°
è´Ÿè´£ç½‘é¡µå†…å®¹æŠ“å–å’Œä¿¡æ¯æå–
"""

from typing import Dict, Any, Optional, List
from agents.base import BaseAgent, AgentConfig
from utils.logger import get_logger

logger = get_logger("CrawlAgent")


class CrawlAgent(BaseAgent):
    """
    Crawl Agent - å†…å®¹æŠ“å–ä¸“å®¶
    
    æ ¸å¿ƒèƒ½åŠ›ï¼š
    1. æ·±åº¦å†…å®¹æŠ“å–ï¼šä»æŒ‡å®šURLæå–è¯¦ç»†ä¿¡æ¯
    2. æ™ºèƒ½å†…å®¹æ¸…æ´—ï¼šå»é™¤æ— å…³å†…å®¹ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯
    3. ç»“æ„åŒ–æå–ï¼šè¯†åˆ«å¹¶æå–å…³é”®ä¿¡æ¯ç‚¹
    4. è´¨é‡ä¼˜å…ˆï¼šæ³¨é‡å†…å®¹è´¨é‡è€Œéæ•°é‡
    
    å·¥å…·é…ç½®ï¼š
    - web_fetch: ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·
    """
    
    def __init__(self, config: Optional[AgentConfig] = None, toolkit=None):
        """
        åˆå§‹åŒ–Crawl Agent
        
        Args:
            config: Agenté…ç½®
            toolkit: å·¥å…·åŒ…ï¼ˆåº”åŒ…å«web_fetchå·¥å…·ï¼‰
        """
        if not config:
            config = AgentConfig(
                name="crawl_agent",
                description="Web content extraction and analysis specialist",
                model="qwen-plus",
                temperature=0.3,  # æ›´ä½æ¸©åº¦forç²¾ç¡®æå–
                max_tool_rounds=2,  # é€šå¸¸1-2è½®å³å¯
                streaming=True
            )
        
        super().__init__(config, toolkit)
    
    def build_system_prompt(self, context: Optional[Dict[str, Any]] = None) -> str:
        """
        æ„å»ºCrawl Agentçš„ç³»ç»Ÿæç¤ºè¯
        
        Args:
            context: åŒ…å«URLåˆ—è¡¨å’Œæå–è¦æ±‚çš„ä¸Šä¸‹æ–‡
            
        Returns:
            ç³»ç»Ÿæç¤ºè¯
        """
        prompt = f"""You are {self.config.name}, a specialized agent for web content extraction and analysis.

## Your Mission

Extract, clean, and structure valuable information from web pages for research purposes.

## Core Capabilities

### 1. Content Extraction
- Fetch complete page content
- Identify main article/content area
- Extract text, data, and key information
- Preserve important context and relationships

### 2. Content Cleaning
- Remove navigation, ads, and boilerplate
- Filter out irrelevant sections
- Keep only substantive content
- Maintain logical flow and structure

### 3. Information Structuring
Extract and organize:
- Main topics and themes
- Key facts and figures
- Important quotes and statements
- Data points and statistics
- Conclusions and insights

### 4. Quality Assessment
Evaluate content based on:
- Relevance to research topic
- Information density
- Source authority
- Content freshness
- Factual accuracy

## Extraction Process

1. **Receive URLs**: Get list of pages to analyze
2. **Fetch Content**: Use web_fetch tool
3. **Analyze Structure**: Understand page organization
4. **Extract Key Info**: Pull out relevant information
5. **Format Results**: Structure findings in XML

## Output Format

Return extracted content in this XML structure:

```xml
<extraction_results>
  <summary>Overview of extracted content</summary>
  <pages>
    <page>
      <url>https://...</url>
      <title>Page Title</title>
      <extracted_at>2024-XX-XX</extracted_at>
      <key_content>
        <section name="Main Topic">
          <p>Important paragraph or section content</p>
          <facts>
            <fact>Key fact or data point</fact>
            <fact>Another important finding</fact>
          </facts>
        </section>
        <!-- More sections -->
      </key_content>
      <metadata>
        <author>If available</author>
        <publish_date>If available</publish_date>
        <word_count>Approximate count</word_count>
      </metadata>
    </page>
    <!-- More pages -->
  </pages>
  <extraction_stats>
    <total_pages>X</total_pages>
    <successful>X</successful>
    <failed>X</failed>
  </extraction_stats>
</extraction_results>
```

## Extraction Guidelines

- Focus on substantive content over metadata
- Preserve important context and relationships
- Summarize long sections while keeping key points
- Maintain factual accuracy - don't infer or add information
- Handle failed fetches gracefully

## Tool Usage

You have access to the web_fetch tool with these parameters:
- urls: Single URL or list of URLs (required)
- max_content_length: Maximum content per page (default 5000)
- max_concurrent: Concurrent fetches (default 3, max 5)

## Special Instructions

- Quality over quantity: Better to extract less but more relevant content
- Don't attempt to fetch the same URL multiple times
- If content is behind paywall or restricted, note it and move on
- For very long pages, focus on the most relevant sections"""
        
        # æ·»åŠ ä»»åŠ¡ä¸Šä¸‹æ–‡
        if context:
            if context.get("urls"):
                prompt += "\n\n## URLs to Process"
                for url in context["urls"]:
                    prompt += f"\n- {url}"
            
            if context.get("focus_areas"):
                prompt += "\n\n## Focus Areas"
                for area in context["focus_areas"]:
                    prompt += f"\n- {area}"
            
            if context.get("task_plan"):
                prompt += f"\n\n## Research Context\n{context['task_plan']}"
        
        return prompt
    
    def format_final_response(self, content: str, tool_history: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–Crawl Agentçš„æœ€ç»ˆå“åº”ä¸ºXML
        
        Args:
            content: LLMçš„åˆ†æå’Œæ€»ç»“
            tool_history: æŠ“å–å·¥å…·è°ƒç”¨å†å²
            
        Returns:
            XMLæ ¼å¼çš„æå–ç»“æœ
        """
        # å¦‚æœcontentå·²ç»æ˜¯XMLæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if "<extraction_results>" in content:
            return content
        
        # å¦åˆ™æ„å»ºæ ‡å‡†XMLå“åº”
        xml_parts = ["<extraction_results>"]
        
        # æ·»åŠ æ‘˜è¦
        xml_parts.append(f"  <summary>{self._extract_summary(content)}</summary>")
        
        # æ·»åŠ æŠ“å–çš„é¡µé¢å†…å®¹
        xml_parts.append("  <pages>")
        
        successful = 0
        failed = 0
        
        for call in tool_history:
            if call["tool"] == "web_fetch":
                result = call["result"]
                if result["success"]:
                    successful += 1
                    # æå–å’Œæ ¼å¼åŒ–å†…å®¹
                    fetch_data = result.get("data", "")
                    xml_parts.append(self._format_fetch_data(fetch_data, call["params"]))
                else:
                    failed += 1
        
        xml_parts.append("  </pages>")
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        xml_parts.append("  <extraction_stats>")
        xml_parts.append(f"    <total_pages>{successful + failed}</total_pages>")
        xml_parts.append(f"    <successful>{successful}</successful>")
        xml_parts.append(f"    <failed>{failed}</failed>")
        xml_parts.append("  </extraction_stats>")
        
        xml_parts.append("</extraction_results>")
        
        return "\n".join(xml_parts)
    
    def _extract_summary(self, content: str) -> str:
        """ä»å†…å®¹ä¸­æå–æ‘˜è¦"""
        lines = content.strip().split('\n')
        summary = lines[0] if lines else content
        
        if len(summary) > 300:
            summary = summary[:297] + "..."
        
        return summary
    
    def _format_fetch_data(self, fetch_data: str, params: Dict) -> str:
        """æ ¼å¼åŒ–æŠ“å–æ•°æ®ä¸ºXMLç‰‡æ®µ"""
        # è§£æfetch_dataä¸­çš„å†…å®¹
        xml_parts = ["    <page>"]
        
        # è·å–URL
        urls = params.get("urls", [])
        if isinstance(urls, str):
            urls = [urls]
        
        if urls:
            xml_parts.append(f"      <url>{urls[0]}</url>")
        
        # å°è¯•ä»fetch_dataæå–æ ‡é¢˜å’Œå†…å®¹
        if isinstance(fetch_data, str):
            # ç®€å•æå–ï¼ˆå®é™…åº”è¯¥è§£æXMLï¼‰
            if "<title>" in fetch_data:
                import re
                title_match = re.search(r'<title>(.*?)</title>', fetch_data)
                if title_match:
                    xml_parts.append(f"      <title>{title_match.group(1)}</title>")
            else:
                xml_parts.append("      <title>Untitled</title>")
            
            # æå–å†…å®¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            xml_parts.append("      <key_content>")
            
            # æˆªå–ä¸»è¦å†…å®¹
            content_preview = fetch_data[:1000] if len(fetch_data) > 1000 else fetch_data
            xml_parts.append(f"        <section name=\"Main Content\">")
            xml_parts.append(f"          <p>{self._clean_text(content_preview)}</p>")
            xml_parts.append(f"        </section>")
            
            xml_parts.append("      </key_content>")
        
        xml_parts.append("    </page>")
        
        return "\n".join(xml_parts)
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦
        import re
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # è½¬ä¹‰XMLç‰¹æ®Šå­—ç¬¦
        replacements = {
            "&": "&amp;",
            "<": "&lt;",
            ">": "&gt;",
            '"': "&quot;",
            "'": "&apos;"
        }
        
        for char, escaped in replacements.items():
            text = text.replace(char, escaped)
        
        return text
    
    async def extract_from_urls(
        self,
        urls: List[str],
        focus_areas: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        ä»URLåˆ—è¡¨æå–å†…å®¹
        
        Args:
            urls: è¦æŠ“å–çš„URLåˆ—è¡¨
            focus_areas: å…³æ³¨çš„å†…å®¹é¢†åŸŸ
            
        Returns:
            æå–ç»“æœå­—å…¸
        """
        context = {
            "urls": urls,
            "focus_areas": focus_areas or []
        }
        
        # æ„å»ºæŒ‡ä»¤
        instruction = f"Please extract and analyze content from the following {len(urls)} URL(s)."
        if focus_areas:
            instruction += f" Focus on: {', '.join(focus_areas)}"
        
        # ä½¿ç”¨åŸºç±»çš„executeæ–¹æ³•
        response = await self.execute(instruction, context)
        
        return {
            "success": True,
            "extracted_content": response.content,
            "tool_calls": response.tool_calls,
            "pages_processed": len([c for c in response.tool_calls 
                                   if c.get("tool") == "web_fetch"])
        }


# å·¥å‚å‡½æ•°
def create_crawl_agent(toolkit=None) -> CrawlAgent:
    """
    åˆ›å»ºCrawl Agentå®ä¾‹
    
    Args:
        toolkit: åŒ…å«web_fetchå·¥å…·çš„å·¥å…·åŒ…
        
    Returns:
        é…ç½®å¥½çš„Crawl Agentå®ä¾‹
    """
    return CrawlAgent(toolkit=toolkit)


if __name__ == "__main__":
    import asyncio
    
    async def test_crawl_agent():
        """æµ‹è¯•Crawl AgentåŸºç¡€åŠŸèƒ½"""
        print("\nğŸ§ª Testing Crawl Agent")
        print("="*50)
        
        # åˆ›å»ºCrawl Agent
        agent = create_crawl_agent()
        
        # æµ‹è¯•1: ç³»ç»Ÿæç¤ºè¯
        print("\nğŸ“ System Prompt (excerpt):")
        context = {
            "urls": ["https://example.com/article1", "https://example.com/article2"],
            "focus_areas": ["Key findings", "Statistical data"],
            "task_plan": "Research on AI safety"
        }
        prompt = agent.build_system_prompt(context)
        print(prompt[:800] + "...")
        
        # æµ‹è¯•2: å“åº”æ ¼å¼åŒ–
        print("\nğŸ“ Response Formatting:")
        mock_tool_history = [
            {
                "tool": "web_fetch",
                "params": {"urls": ["https://example.com"]},
                "result": {
                    "success": True,
                    "data": "<title>Test Page</title><content>Sample content here</content>"
                }
            }
        ]
        formatted = agent.format_final_response("Extracted content from web page", mock_tool_history)
        print(formatted[:600])
        
        print("\nâœ… Crawl Agent tests completed")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_crawl_agent())
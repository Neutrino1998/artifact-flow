"""
Base AgentæŠ½è±¡ç±»
æä¾›Agentçš„åŸºç¡€åŠŸèƒ½ï¼šå·¥å…·è°ƒç”¨å¾ªç¯ã€æµå¼è¾“å‡ºã€ç»Ÿä¸€å®Œæˆåˆ¤æ–­ç­‰
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, AsyncGenerator, Union
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

from utils.logger import get_logger
from utils.xml_parser import parse_tool_calls
from tools.prompt_generator import ToolPromptGenerator, format_result
from tools.registry import AgentToolkit
from tools.base import ToolResult

logger = get_logger("Agents")


class StreamEventType(Enum):
    """æµå¼äº‹ä»¶ç±»å‹"""
    START = "start"                # å¼€å§‹æ‰§è¡Œ
    LLM_CHUNK = "llm_chunk"        # LLMè¾“å‡ºç‰‡æ®µ
    LLM_COMPLETE = "llm_complete"  # LLMè¾“å‡ºå®Œæˆ
    TOOL_START = "tool_start"      # å·¥å…·è°ƒç”¨å¼€å§‹
    TOOL_RESULT = "tool_result"    # å·¥å…·è°ƒç”¨ç»“æœ
    COMPLETE = "complete"          # æ‰§è¡Œå®Œæˆ
    ERROR = "error"                # é”™è¯¯


@dataclass
class StreamEvent:
    """æµå¼äº‹ä»¶"""
    type: StreamEventType
    agent: str
    timestamp: datetime = field(default_factory=datetime.now)
    data: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentConfig:
    """Agenté…ç½®"""
    name: str
    description: str
    model: str = "qwen-plus"
    temperature: float = 0.7
    max_tool_rounds: int = 3  # æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°
    streaming: bool = True  # æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º
    debug: bool = False  # æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼


@dataclass
class AgentResponse:
    """Agentå“åº”"""
    content: str  # æœ€ç»ˆå›å¤å†…å®¹
    tool_calls: List[Dict[str, Any]] = field(default_factory=list)  # å·¥å…·è°ƒç”¨è®°å½•
    reasoning_content: Optional[str] = None  # æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
    metadata: Dict[str, Any] = field(default_factory=dict)  # å…ƒæ•°æ®
    routing: Optional[Dict[str, Any]] = None  # æ–°å¢ï¼šè·¯ç”±ä¿¡æ¯

class BaseAgent(ABC):
    """
    æ‰€æœ‰Agentçš„åŸºç±»
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨å¾ªç¯ï¼ˆæœ€å¤š3è½®ï¼‰
    2. æµå¼è¾“å‡ºæ”¯æŒï¼ˆLLMæµå¼ï¼Œå·¥å…·æ‰¹é‡ï¼‰
    3. ç»Ÿä¸€çš„å®Œæˆåˆ¤æ–­ï¼ˆæ— å·¥å…·è°ƒç”¨å³å®Œæˆï¼‰
    4. æ€è€ƒæ¨¡å‹å…¼å®¹ï¼ˆè®°å½•reasoning_contentï¼‰
    """
    
    def __init__(self, config: AgentConfig, toolkit: Optional[AgentToolkit] = None):
        """
        åˆå§‹åŒ–Agent
        
        Args:
            config: Agenté…ç½®
            toolkit: å·¥å…·åŒ…ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config
        self.toolkit = toolkit
        self.tool_call_count = 0  # å·¥å…·è°ƒç”¨è®¡æ•°
        self.conversation_history = []  # å¯¹è¯å†å²
        
        # åˆ›å»ºLLMå®ä¾‹
        from models.llm import create_llm
        self.llm = create_llm(
            model=config.model,
            temperature=config.temperature,
            streaming=config.streaming
        )
        
        logger.info(f"Initialized {config.name} with model {config.model}")

    def _format_messages_for_debug(self, messages: List[Dict], max_content_len: int = 100000) -> str:
        """å°†messagesæ ¼å¼åŒ–ä¸ºç®€æ´çš„èŠå¤©è®°å½•æ ¼å¼"""
        formatted_lines = []
        
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            
            # æˆªæ–­é•¿å†…å®¹
            if len(content) > max_content_len:
                content = content[:max_content_len] + "..."
            
            # æ ¼å¼åŒ–ä¸ºèŠå¤©è®°å½•æ ¼å¼
            formatted_lines.append(f"> {role}:")
            # æ·»åŠ ç¼©è¿›
            for line in content.split('\n'):
                formatted_lines.append(f"  {line}")
            formatted_lines.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(formatted_lines)
    
    @abstractmethod
    def build_system_prompt(self, context: Optional[Dict[str, Any]] = None) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆå­ç±»å®ç°ï¼‰
        
        Args:
            context: åŠ¨æ€ä¸Šä¸‹æ–‡ï¼ˆå¦‚task_planå†…å®¹ï¼‰
            
        Returns:
            ç³»ç»Ÿæç¤ºè¯
        """
        pass
    
    @abstractmethod
    def format_final_response(self, content: str, tool_history: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æœ€ç»ˆå“åº”ï¼ˆå­ç±»å®ç°ï¼‰
        
        Args:
            content: LLMçš„æœ€ç»ˆå›å¤
            tool_history: å·¥å…·è°ƒç”¨å†å²
            
        Returns:
            æ ¼å¼åŒ–åçš„å“åº”
        """
        pass
    
    async def execute(
        self,
        user_input: str,
        context: Optional[Dict[str, Any]] = None
    ) -> AgentResponse:
        """
        æ‰§è¡ŒAgentä»»åŠ¡ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        
        å®ç°ç»Ÿä¸€çš„æ‰§è¡Œæµç¨‹ï¼š
        1. æ„å»ºæç¤ºè¯
        2. å·¥å…·è°ƒç”¨å¾ªç¯ï¼ˆæœ€å¤š3è½®ï¼‰
        3. ç»Ÿä¸€çš„å®Œæˆåˆ¤æ–­
        4. è¿”å›æ ¼å¼åŒ–ç»“æœ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥/ä»»åŠ¡æŒ‡ä»¤
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Returns:
            Agentå“åº”
        """
        # é‡ç½®çŠ¶æ€
        self.tool_call_count = 0
        tool_history = []
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯
        system_prompt = self.build_system_prompt(context)
        
        # æ·»åŠ å·¥å…·ä½¿ç”¨è¯´æ˜ï¼ˆå¦‚æœæœ‰å·¥å…·ï¼‰
        if self.toolkit and self.toolkit.list_tools():
            tools_instruction = ToolPromptGenerator.generate_tool_instruction(
                self.toolkit.list_tools()
            )
            system_prompt += f"\n\n{tools_instruction}"
        
        # å‡†å¤‡æ¶ˆæ¯
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]
        
        # å·¥å…·è°ƒç”¨å¾ªç¯
        final_content = ""
        reasoning_content = None
        
        for round_num in range(self.config.max_tool_rounds + 1):
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å·¥å…·è°ƒç”¨é™åˆ¶
            if round_num == self.config.max_tool_rounds:
                # æ·»åŠ é™åˆ¶æç¤º
                messages.append({
                    "role": "system",
                    "content": "âš ï¸ You have reached the maximum tool call limit. Please summarize your findings and provide the final response."
                })
            # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•å®Œæ•´çš„messages
            if self.config.debug:
                logger.debug(f"[{self.config.name} Round {round_num + 1}] =================")
                logger.debug(f"[{self.config.name} Round {round_num + 1}] Messages being sent to LLM:\n{self._format_messages_for_debug(messages)}")
                logger.debug(f"[{self.config.name} Round {round_num + 1}] =================")

            # è°ƒç”¨LLM
            if self.config.streaming:
                # æµå¼è¾“å‡ºï¼ˆç”¨æˆ·å¯å®æ—¶çœ‹åˆ°ï¼‰
                response_content = ""
                async for chunk in self.llm.astream(messages):
                    if hasattr(chunk, 'content'):
                        response_content += chunk.content
                        # TODO: è¿™é‡Œå¯ä»¥yield chunkç»™å‰ç«¯
                    
                    # è®°å½•æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                    if hasattr(chunk, 'additional_kwargs'):
                        if 'reasoning_content' in chunk.additional_kwargs:
                            reasoning_content = chunk.additional_kwargs['reasoning_content']
            else:
                # æ‰¹é‡è¾“å‡º
                response = await self.llm.ainvoke(messages)
                response_content = response.content
                
                # è®°å½•æ€è€ƒè¿‡ç¨‹
                if hasattr(response, 'additional_kwargs'):
                    if 'reasoning_content' in response.additional_kwargs:
                        reasoning_content = response.additional_kwargs['reasoning_content']
            
            # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•å®Œæ•´å¯¹è¯
            if self.config.debug:
                if reasoning_content:
                    logger.debug(f"[{self.config.name} Round {round_num + 1}] Reasoning (complete):\n{reasoning_content}")
                logger.debug(f"[{self.config.name} Round {round_num + 1}] LLM Response (complete):\n{response_content}")

            # è§£æå·¥å…·è°ƒç”¨
            tool_calls = parse_tool_calls(response_content)
            
            # åˆ¤æ–­æ˜¯å¦å®Œæˆï¼ˆæ— å·¥å…·è°ƒç”¨å³å®Œæˆï¼‰
            if not tool_calls or round_num >= self.config.max_tool_rounds:
                final_content = response_content
                break
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_results = []
            for tool_call in tool_calls:
                self.tool_call_count += 1
                
                logger.info(f"{self.config.name} calling tool: {tool_call.name}")
                
                # æ‰§è¡Œå·¥å…·
                if self.toolkit:
                    result = await self.toolkit.execute_tool(
                        tool_call.name,
                        tool_call.params
                    )
                    
                    # è®°å½•å·¥å…·è°ƒç”¨å†å²
                    tool_history.append({
                        "tool": tool_call.name,
                        "params": tool_call.params,
                        "result": result.to_dict()
                    })
                    
                    # ğŸ” æ£€æŸ¥æ˜¯å¦æ˜¯è·¯ç”±æŒ‡ä»¤
                    if tool_call.name == "call_subagent" and result.success:
                        result_data = result.to_dict().get("data", {})
                        if result_data.get("_is_routing_instruction"):
                            # ç«‹å³è¿”å›ï¼Œå¸¦ä¸Šè·¯ç”±ä¿¡æ¯
                            return AgentResponse(
                                content=response_content,  # å½“å‰çš„å“åº”å†…å®¹
                                tool_calls=tool_history,
                                reasoning_content=reasoning_content,
                                routing={  # è·¯ç”±ä¿¡æ¯
                                    "target": result_data.get("_route_to"),
                                    "instruction": result_data.get("instruction"),
                                    "from_agent": self.config.name
                                },
                                metadata={
                                    "agent": self.config.name,
                                    "model": self.config.model,
                                    "needs_routing": True
                                }
                            )

                    # æ ¼å¼åŒ–å·¥å…·ç»“æœä¸ºXML
                    xml_result = format_result(tool_call.name, result.to_dict())
                    tool_results.append(xml_result)
                else:
                    # æ²¡æœ‰å·¥å…·åŒ…ï¼Œè¿”å›é”™è¯¯
                    tool_results.append(
                        f"<tool_result><name>{tool_call.name}</name>"
                        f"<success>false</success>"
                        f"<error>No toolkit available</error></tool_result>"
                    )
            
            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
            messages.append({"role": "assistant", "content": response_content})
            messages.append({"role": "user", "content": "\n".join(tool_results)})
        
        # æ ¼å¼åŒ–æœ€ç»ˆå“åº”
        formatted_response = self.format_final_response(final_content, tool_history)
        
        # æ„å»ºå“åº”å¯¹è±¡
        return AgentResponse(
            content=formatted_response,
            tool_calls=tool_history,
            reasoning_content=reasoning_content,
            metadata={
                "agent": self.config.name,
                "model": self.config.model,
                "tool_rounds": self.tool_call_count,
                "completed": True
            }
        )
    
    async def execute_stream(
        self,
        user_input: str,
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[StreamEvent, None]:
        """
        æµå¼æ‰§è¡ŒAgentä»»åŠ¡ï¼ˆç”¨äºLangGraphèŠ‚ç‚¹ï¼‰
        
        Yieldsä¸åŒç±»å‹çš„äº‹ä»¶ï¼Œæ”¯æŒå®æ—¶æµå¼è¾“å‡º
        
        äº‹ä»¶ç±»å‹ï¼š
        - START: æ‰§è¡Œå¼€å§‹
        - LLM_CHUNK: LLMè¾“å‡ºç‰‡æ®µ
        - LLM_COMPLETE: LLMè¾“å‡ºå®Œæˆ
        - TOOL_START: å·¥å…·è°ƒç”¨å¼€å§‹
        - TOOL_RESULT: å·¥å…·è°ƒç”¨ç»“æœ
        - COMPLETE: æ‰§è¡Œå®Œæˆ
        - ERROR: é”™è¯¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥/ä»»åŠ¡æŒ‡ä»¤
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        # Yieldå¼€å§‹äº‹ä»¶
        yield StreamEvent(
            type=StreamEventType.START,
            agent=self.config.name,
            data={"user_input": user_input[:100], "has_context": context is not None}
        )
        
        # é‡ç½®çŠ¶æ€
        self.tool_call_count = 0
        tool_history = []
        
        try:
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = self.build_system_prompt(context)
            
            # æ·»åŠ å·¥å…·ä½¿ç”¨è¯´æ˜ï¼ˆå¦‚æœæœ‰å·¥å…·ï¼‰
            if self.toolkit and self.toolkit.list_tools():
                tools_instruction = ToolPromptGenerator.generate_tool_instruction(
                    self.toolkit.list_tools()
                )
                system_prompt += f"\n\n{tools_instruction}"
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_input}
            ]
            
            # å·¥å…·è°ƒç”¨å¾ªç¯
            final_content = ""
            reasoning_content = None
            
            for round_num in range(self.config.max_tool_rounds + 1):
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å·¥å…·è°ƒç”¨é™åˆ¶
                if round_num == self.config.max_tool_rounds:
                    messages.append({
                        "role": "system",
                        "content": "âš ï¸ You have reached the maximum tool call limit. Please summarize your findings and provide the final response."
                    })
                # è°ƒè¯•æ¨¡å¼ï¼šè®°å½•å®Œæ•´çš„messages
                if self.config.debug:
                    logger.debug(f"[{self.config.name} Round {round_num + 1}] =================")
                    logger.debug(f"[{self.config.name} Round {round_num + 1}] Messages being sent to LLM:\n{self._format_messages_for_debug(messages)}")
                    logger.debug(f"[{self.config.name} Round {round_num + 1}] =================")

                # LLMæµå¼è°ƒç”¨
                response_content = ""
                chunk_count = 0
                
                if self.config.streaming:
                    # æµå¼è¾“å‡º
                    async for chunk in self.llm.astream(messages):
                        if hasattr(chunk, 'content') and chunk.content:
                            response_content += chunk.content
                            chunk_count += 1
                            
                            # Yield LLM chunkäº‹ä»¶
                            yield StreamEvent(
                                type=StreamEventType.LLM_CHUNK,
                                agent=self.config.name,
                                data={
                                    "content": chunk.content,
                                    "round": round_num + 1,
                                    "chunk_index": chunk_count
                                }
                            )
                        
                        # è®°å½•æ€è€ƒè¿‡ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(chunk, 'additional_kwargs'):
                            if 'reasoning_content' in chunk.additional_kwargs:
                                reasoning_content = chunk.additional_kwargs['reasoning_content']
                                # å¯é€‰ï¼šyieldæ€è€ƒå†…å®¹
                                if self.config.debug:
                                    yield StreamEvent(
                                        type=StreamEventType.LLM_CHUNK,
                                        agent=self.config.name,
                                        data={
                                            "thinking": reasoning_content,
                                            "is_thinking": True
                                        }
                                    )
                else:
                    # æ‰¹é‡è¾“å‡ºï¼ˆéæµå¼ï¼‰
                    response = await self.llm.ainvoke(messages)
                    response_content = response.content
                    
                    # Yieldå®Œæ•´çš„LLMå“åº”ä½œä¸ºå•ä¸ªchunk
                    yield StreamEvent(
                        type=StreamEventType.LLM_CHUNK,
                        agent=self.config.name,
                        data={
                            "content": response_content,
                            "round": round_num + 1,
                            "is_complete": True
                        }
                    )
                    
                    # è®°å½•æ€è€ƒè¿‡ç¨‹
                    if hasattr(response, 'additional_kwargs'):
                        if 'reasoning_content' in response.additional_kwargs:
                            reasoning_content = response.additional_kwargs['reasoning_content']
                
                # Yield LLMå®Œæˆäº‹ä»¶
                yield StreamEvent(
                    type=StreamEventType.LLM_COMPLETE,
                    agent=self.config.name,
                    data={
                        "round": round_num + 1,
                        "content_length": len(response_content)
                    }
                )
                
                # è°ƒè¯•æ¨¡å¼è®°å½•
                if self.config.debug:
                    if reasoning_content:
                        logger.debug(f"[{self.config.name} Round {round_num + 1}] Reasoning (complete):\n{reasoning_content}")
                    logger.debug(f"[{self.config.name} Round {round_num + 1}] LLM Response (complete):\n{response_content}")

                # è§£æå·¥å…·è°ƒç”¨
                tool_calls = parse_tool_calls(response_content)
                
                # åˆ¤æ–­æ˜¯å¦å®Œæˆï¼ˆæ— å·¥å…·è°ƒç”¨å³å®Œæˆï¼‰
                if not tool_calls or round_num >= self.config.max_tool_rounds:
                    final_content = response_content
                    break
                
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_results = []
                for tool_call in tool_calls:
                    self.tool_call_count += 1
                    
                    # Yieldå·¥å…·å¼€å§‹äº‹ä»¶
                    yield StreamEvent(
                        type=StreamEventType.TOOL_START,
                        agent=self.config.name,
                        data={
                            "tool": tool_call.name,
                            "params": tool_call.params,
                            "round": round_num + 1,
                            "call_index": self.tool_call_count
                        }
                    )
                    
                    logger.info(f"{self.config.name} calling tool: {tool_call.name}")
                    
                    # æ‰§è¡Œå·¥å…·
                    if self.toolkit:
                        result = await self.toolkit.execute_tool(
                            tool_call.name,
                            tool_call.params
                        )
                        
                        # è®°å½•å·¥å…·è°ƒç”¨å†å²
                        tool_history.append({
                            "tool": tool_call.name,
                            "params": tool_call.params,
                            "result": result.to_dict()
                        })
                        
                        # Yieldå·¥å…·ç»“æœäº‹ä»¶
                        yield StreamEvent(
                            type=StreamEventType.TOOL_RESULT,
                            agent=self.config.name,
                            data={
                                "tool": tool_call.name,
                                "success": result.success,
                                "result": result.to_dict(),
                                "round": round_num + 1
                            }
                        )
                        
                        # æ ¼å¼åŒ–å·¥å…·ç»“æœä¸ºXML
                        xml_result = format_result(tool_call.name, result.to_dict())
                        tool_results.append(xml_result)
                    else:
                        # æ²¡æœ‰å·¥å…·åŒ…ï¼Œè¿”å›é”™è¯¯
                        error_result = {
                            "success": False,
                            "error": "No toolkit available"
                        }
                        
                        yield StreamEvent(
                            type=StreamEventType.TOOL_RESULT,
                            agent=self.config.name,
                            data={
                                "tool": tool_call.name,
                                "success": False,
                                "error": "No toolkit available"
                            }
                        )
                        
                        tool_results.append(
                            f"<tool_result><name>{tool_call.name}</name>"
                            f"<success>false</success>"
                            f"<error>No toolkit available</error></tool_result>"
                        )
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å¯¹è¯å†å²
                messages.append({"role": "assistant", "content": response_content})
                messages.append({"role": "user", "content": "\n".join(tool_results)})
            
            # æ ¼å¼åŒ–æœ€ç»ˆå“åº”
            formatted_response = self.format_final_response(final_content, tool_history)
            
            # æ„å»ºæœ€ç»ˆå“åº”å¯¹è±¡
            final_response = AgentResponse(
                content=formatted_response,
                tool_calls=tool_history,
                reasoning_content=reasoning_content,
                metadata={
                    "agent": self.config.name,
                    "model": self.config.model,
                    "tool_rounds": self.tool_call_count,
                    "completed": True
                }
            )
            
            # Yieldå®Œæˆäº‹ä»¶
            yield StreamEvent(
                type=StreamEventType.COMPLETE,
                agent=self.config.name,
                data={
                    "response": final_response,
                    "tool_calls_count": len(tool_history),
                    "final_content_length": len(formatted_response)
                }
            )
            
        except Exception as e:
            # Yieldé”™è¯¯äº‹ä»¶
            logger.exception(f"Agent execution error: {str(e)}")
            yield StreamEvent(
                type=StreamEventType.ERROR,
                agent=self.config.name,
                data={
                    "error": str(e),
                    "error_type": type(e).__name__
                }
            )
            raise
    
    async def reset(self):
        """é‡ç½®AgentçŠ¶æ€"""
        self.tool_call_count = 0
        self.conversation_history.clear()
        logger.debug(f"{self.config.name} state reset")


# ä¾¿æ·å‡½æ•°
def create_agent_config(
    name: str,
    description: str,
    **kwargs
) -> AgentConfig:
    """åˆ›å»ºAgenté…ç½®çš„ä¾¿æ·å‡½æ•°"""
    return AgentConfig(name=name, description=description, **kwargs)
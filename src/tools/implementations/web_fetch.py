"""
Webå†…å®¹æŠ“å–å·¥å…·
åŸºäºcrawl4aiå®ç°ç½‘é¡µå†…å®¹çš„æ·±åº¦æŠ“å–
"""

import asyncio
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

from tools.base import BaseTool, ToolResult, ToolParameter, ToolPermission
from utils.logger import get_logger
from utils.retry import api_retry
import random

# å¯¼å…¥crawl4aiç»„ä»¶
try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    from crawl4ai.content_filter_strategy import PruningContentFilter
    from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
    from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher, RateLimiter
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    logger = get_logger("Tools")
    logger.warning("crawl4ai not installed. Install with: pip install crawl4ai")

logger = get_logger("Tools")


class WebFetchTool(BaseTool):
    """
    Webå†…å®¹æŠ“å–å·¥å…·
    ä½¿ç”¨crawl4aiæ·±åº¦æŠ“å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼
    
    ç‰¹æ€§ï¼š
    - å†…å­˜è‡ªé€‚åº”ï¼šé€šè¿‡MemoryAdaptiveDispatcheræ§åˆ¶å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°
    - é˜²æ­¢å†…å­˜çˆ†ç‚¸ï¼šæ¯ä¸ªURLä¼šå¯åŠ¨ä¸€ä¸ªæµè§ˆå™¨å®ä¾‹ï¼Œéœ€è¦ä¸¥æ ¼æ§åˆ¶å¹¶å‘
    - é™çº§å¤„ç†ï¼šå½“å†…å­˜æ§åˆ¶å™¨ä¸å¯ç”¨æ—¶ï¼Œé™çº§ä¸ºé¡ºåºæŠ“å–
    """
    
    def __init__(self):
        super().__init__(
            name="web_fetch",
            description="Fetch and extract content from web pages",
            permission=ToolPermission.PUBLIC
        )
        
        if not CRAWL4AI_AVAILABLE:
            logger.error("crawl4ai is not available")
            return
        
        # æ–°å¢ï¼šUser-Agent æ± 
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        ]
        
        # åˆå§‹åŒ–æµè§ˆå™¨é…ç½®
        self.browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            user_agent=random.choice(self.user_agents)  # éšæœº User-Agent
        )
        
        # å†…å®¹è¿‡æ»¤å™¨é…ç½®
        self.prune_filter = PruningContentFilter(
            threshold=0.45,              # é€‚ä¸­çš„é˜ˆå€¼ï¼Œå¹³è¡¡å†…å®¹è´¨é‡å’Œæ•°é‡
            threshold_type="dynamic",   # åŠ¨æ€è°ƒæ•´é˜ˆå€¼
            # æ³¨æ„ï¼šä¸è®¾ç½®min_word_thresholdï¼Œé¿å…è¿‡æ»¤æ‰ä¸»ä½“å†…å®¹
        )
        
        # Markdownç”Ÿæˆå™¨é…ç½®
        self.md_generator = DefaultMarkdownGenerator(
            options={
                "ignore_links": True,      # ç§»é™¤è¶…é“¾æ¥
                "ignore_images": True,      # ç§»é™¤å›¾ç‰‡
                "escape_html": True,        # è½¬ä¹‰HTMLå®ä½“
                "skip_internal_links": True # è·³è¿‡å†…éƒ¨é“¾æ¥
            },
            content_filter=self.prune_filter
        )
        
        # è¿è¡Œé…ç½®
        self.run_config = CrawlerRunConfig(
            # å†…å®¹è¿‡æ»¤
            word_count_threshold=100,  # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå†…å®¹
            excluded_tags=['form', 'header', 'footer', 'nav'],  
            exclude_external_links=True,
            # å†…å®¹å¤„ç†
            process_iframes=True,
            remove_overlay_elements=True,
            # ç¼“å­˜æ§åˆ¶
            cache_mode=CacheMode.DISABLED,  # ç¦ç”¨ç¼“å­˜ï¼Œä¿è¯è·å–æœ€æ–°å†…å®¹
            # Markdownç”Ÿæˆå™¨
            markdown_generator=self.md_generator,
            # ç¦ç”¨æ—¥å¿—
            verbose=False
        )
    
    def get_parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="urls",
                type="array[string]",
                description="URL or list of URLs to fetch",
                required=True
            ),
            ToolParameter(
                name="max_content_length",
                type="integer",
                description="Maximum content length per page in characters (default: 5000)",
                required=False,
                default=5000
            ),
            ToolParameter(
                name="max_concurrent",
                type="integer",
                description="Maximum concurrent browser instances (default: 3, max: 5) - Each browser uses ~100-300MB memory",
                required=False,
                default=3
            )
        ]
    
    async def execute(self, **params) -> ToolResult:
        """
        æ‰§è¡Œç½‘é¡µæŠ“å–
        
        Args:
            urls: URLå­—ç¬¦ä¸²æˆ–URLåˆ—è¡¨
            max_content_length: æ¯é¡µæœ€å¤§å†…å®¹é•¿åº¦
            max_concurrent: æœ€å¤§å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°
            
        Returns:
            ToolResult: åŒ…å«XMLæ ¼å¼çš„æŠ“å–ç»“æœ
        """
        if not CRAWL4AI_AVAILABLE:
            return ToolResult(
                success=False,
                error="crawl4ai is not installed. Please install it first."
            )
        
        # å‚æ•°å¤„ç†
        urls_param = params.get("urls")
        if not urls_param:
            return ToolResult(success=False, error="urls parameter is required")
        
        # ç¡®ä¿urlsæ˜¯åˆ—è¡¨
        if isinstance(urls_param, str):
            urls = [urls_param]
        elif isinstance(urls_param, list):
            urls = urls_param
        else:
            return ToolResult(success=False, error="urls must be string or list")
        
        max_content_length = params.get("max_content_length", 5000)
        max_concurrent = min(params.get("max_concurrent", 3), 5)  # é™åˆ¶æœ€å¤§5ä¸ª
        
        logger.info(f"Fetching {len(urls)} URL(s) with max {max_concurrent} concurrent browsers")
        
        try:
            # æ‰§è¡ŒæŠ“å–
            results = await self._fetch_urls(urls, max_content_length, max_concurrent)
            
            # æ ¼å¼åŒ–ä¸ºXML
            xml_result = self._format_results_to_xml(results)
            
            # ç»Ÿè®¡ä¿¡æ¯
            success_count = sum(1 for r in results if r.get("success"))
            
            logger.info(f"Fetch completed: {success_count}/{len(urls)} successful")
            
            return ToolResult(
                success=True,
                data=xml_result,
                metadata={
                    "total_urls": len(urls),
                    "success_count": success_count,
                    "failed_count": len(urls) - success_count
                }
            )
            
        except Exception as e:
            logger.exception(f"Fetch failed: {str(e)}")
            return ToolResult(success=False, error=f"Fetch failed: {str(e)}")
    
    async def _fetch_urls(
        self,
        urls: List[str],
        max_content_length: int,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æŠ“å–å¤šä¸ªURL
        
        Args:
            urls: URLåˆ—è¡¨
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
            max_concurrent: æœ€å¤§å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°
            
        Returns:
            æŠ“å–ç»“æœåˆ—è¡¨
        """
        results = []
        
        # åˆ›å»ºå†…å­˜è‡ªé€‚åº”è°ƒåº¦å™¨ - é˜²æ­¢å†…å­˜çˆ†ç‚¸
        dispatcher = None
        if CRAWL4AI_AVAILABLE and 'MemoryAdaptiveDispatcher' in globals():
            dispatcher = MemoryAdaptiveDispatcher(
                memory_threshold_percent=70.0,          # å†…å­˜è¶…è¿‡70%æ—¶æš‚åœ
                check_interval=1.0,                      # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡å†…å­˜
                max_session_permit=max_concurrent,      # æœ€å¤§å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°
                memory_wait_timeout=120.0,              # è¶…æ—¶120ç§’æŠ›å‡ºé”™è¯¯
                rate_limiter=RateLimiter(
                    base_delay=(0.5, 1.0),               # åŸºç¡€å»¶è¿Ÿ0.5-1ç§’
                    max_delay=10.0,                      # æœ€å¤§å»¶è¿Ÿ10ç§’
                    max_retries=2                        # æœ€å¤šé‡è¯•2æ¬¡
                ),
            )
            logger.debug(f"Using MemoryAdaptiveDispatcher with max {max_concurrent} concurrent browsers")
        
        async with AsyncWebCrawler(config=self.browser_config) as crawler:
            # æ‰¹é‡æŠ“å–ï¼ˆå¸¦å†…å­˜æ§åˆ¶ï¼‰
            if dispatcher:
                crawl_results = await crawler.arun_many(
                    urls=urls,
                    config=self.run_config,
                    dispatcher=dispatcher  # ä½¿ç”¨å†…å­˜æ§åˆ¶
                )
            else:
                # å¦‚æœæ²¡æœ‰dispatcherï¼Œé™çº§ä¸ºé¡ºåºæŠ“å–ä»¥é¿å…å†…å­˜é—®é¢˜
                logger.warning("MemoryAdaptiveDispatcher not available, using sequential crawling")
                crawl_results = []
                for url in urls:
                    try:
                        result = await crawler.arun(
                            url=url,
                            config=self.run_config
                        )
                        crawl_results.append(result)
                    except Exception as e:
                        logger.error(f"Failed to crawl {url}: {e}")
                        # åˆ›å»ºä¸€ä¸ªå¤±è´¥çš„ç»“æœå¯¹è±¡
                        class FailedResult:
                            success = False
                            error_message = str(e)
                            metadata = {}
                            fit_markdown = None
                            markdown = None
                        crawl_results.append(FailedResult())
            
            # å¤„ç†ç»“æœ
            for i, result in enumerate(crawl_results):
                url = urls[i] if i < len(urls) else "unknown"
                
                if result.success:
                    # æˆªå–å†…å®¹é•¿åº¦
                    content = result.markdown.fit_markdown or result.markdown or ""
                    if len(content) > max_content_length:
                        content = content[:max_content_length] + "..."
                    
                    results.append({
                        "success": True,
                        "url": url,
                        "title": result.metadata.get("title", "No Title"),
                        "content": content,
                        "word_count": len(content.split()),
                        "fetched_at": datetime.now().isoformat()
                    })
                    
                    logger.debug(f"Successfully fetched {url}: {len(content)} chars")
                else:
                    results.append({
                        "success": False,
                        "url": url,
                        "error": result.error_message or "Unknown error"
                    })
                    
                    logger.warning(f"Failed to fetch {url}: {result.error_message}")
        
        return results
    
    def _format_results_to_xml(self, results: List[Dict[str, Any]]) -> str:
        """
        å°†æŠ“å–ç»“æœæ ¼å¼åŒ–ä¸ºXML
        
        æ ¼å¼ç¤ºä¾‹:
        <fetch_results>
          <fetch_result>
            <url>...</url>
            <title>...</title>
            <content>...</content>
            <word_count>...</word_count>
            <fetched_at>...</fetched_at>
          </fetch_result>
        </fetch_results>
        
        Args:
            results: æŠ“å–ç»“æœåˆ—è¡¨
            
        Returns:
            XMLæ ¼å¼çš„ç»“æœ
        """
        if not results:
            return "<fetch_results>\n  <message>No results</message>\n</fetch_results>"
        
        xml_parts = ["<fetch_results>"]
        
        for result in results:
            if result.get("success"):
                # æˆåŠŸçš„ç»“æœ
                xml_parts.append("  <fetch_result>")
                xml_parts.append(f"    <url>{self._escape_xml(result['url'])}</url>")
                xml_parts.append(f"    <title>{self._escape_xml(result['title'])}</title>")
                xml_parts.append(f"    <content>{self._escape_xml(result['content'])}</content>")
                xml_parts.append(f"    <word_count>{result['word_count']}</word_count>")
                xml_parts.append(f"    <fetched_at>{result['fetched_at']}</fetched_at>")
                xml_parts.append("  </fetch_result>")
            else:
                # å¤±è´¥çš„ç»“æœ
                xml_parts.append("  <fetch_error>")
                xml_parts.append(f"    <url>{self._escape_xml(result['url'])}</url>")
                xml_parts.append(f"    <error>{self._escape_xml(result.get('error', 'Unknown error'))}</error>")
                xml_parts.append("  </fetch_error>")
        
        xml_parts.append("</fetch_results>")
        
        return "\n".join(xml_parts)
    
    def _escape_xml(self, text: str) -> str:
        """
        è½¬ä¹‰XMLç‰¹æ®Šå­—ç¬¦
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            è½¬ä¹‰åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # XMLç‰¹æ®Šå­—ç¬¦è½¬ä¹‰
        replacements = {
            "&": "&amp;",
            "<": "&lt;",
            ">": "&gt;",
            '"': "&quot;",
            "'": "&apos;"
        }
        
        for char, escaped in replacements.items():
            text = text.replace(char, escaped)
        
        return text


# ç®€åŒ–çš„å¤‡ç”¨æŠ“å–å™¨ï¼ˆå½“crawl4aiä¸å¯ç”¨æ—¶ï¼‰
class SimpleFetchTool(BaseTool):
    """
    ç®€å•çš„ç½‘é¡µæŠ“å–å·¥å…·ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
    ä½¿ç”¨aiohttpè¿›è¡ŒåŸºç¡€çš„HTMLæŠ“å–
    """
    
    def __init__(self):
        super().__init__(
            name="web_fetch_simple",
            description="Simple web page fetcher (fallback)",
            permission=ToolPermission.PUBLIC
        )
    
    def get_parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="url",
                type="string",
                description="URL to fetch",
                required=True
            )
        ]
    
    @api_retry()
    async def execute(self, **params) -> ToolResult:
        """ç®€å•çš„HTTP GETè¯·æ±‚"""
        import aiohttp
        from bs4 import BeautifulSoup
        
        url = params.get("url")
        if not url:
            return ToolResult(success=False, error="URL is required")
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status != 200:
                        return ToolResult(
                            success=False,
                            error=f"HTTP {response.status}"
                        )
                    
                    html = await response.text()
                    
                    # åŸºç¡€HTMLè§£æ
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text()
                    lines = (line.strip() for line in text.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    text = ' '.join(chunk for chunk in chunks if chunk)
                    
                    # è·å–æ ‡é¢˜
                    title = soup.title.string if soup.title else "No Title"
                    
                    # é™åˆ¶é•¿åº¦
                    max_length = 5000
                    if len(text) > max_length:
                        text = text[:max_length] + "..."
                    
                    return ToolResult(
                        success=True,
                        data={
                            "url": url,
                            "title": title,
                            "content": text,
                            "length": len(text)
                        }
                    )
                    
        except Exception as e:
            logger.error(f"Simple fetch failed: {str(e)}")
            return ToolResult(success=False, error=str(e))


# æ³¨å†Œå·¥å…·çš„ä¾¿æ·å‡½æ•°
def register_web_fetch_tool():
    """æ³¨å†ŒWebæŠ“å–å·¥å…·"""
    from tools.registry import register_tool
    
    if CRAWL4AI_AVAILABLE:
        register_tool(WebFetchTool())
        logger.info("Registered web fetch tool (crawl4ai)")
    else:
        register_tool(SimpleFetchTool())
        logger.info("Registered web fetch tool (simple fallback)")


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test():
        print("\nğŸ§ª WebæŠ“å–å·¥å…·æµ‹è¯•")
        print("="*50)
        
        if not CRAWL4AI_AVAILABLE:
            print("âš ï¸ crawl4aiæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•å¤‡ç”¨æ–¹æ¡ˆ")
            tool = SimpleFetchTool()
            
            # æµ‹è¯•ç®€å•æŠ“å–
            result = await tool(url="https://github.com/Neutrino1998/artifact-flow")
            if result.success:
                print(f"âœ… æŠ“å–æˆåŠŸ")
                print(f"   æ ‡é¢˜: {result.data['title']}")
                print(f"   å†…å®¹é•¿åº¦: {result.data['length']} å­—ç¬¦")
            else:
                print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")
            return
        
        # ä½¿ç”¨å®Œæ•´çš„crawl4aiå·¥å…·
        tool = WebFetchTool()
        
        # æµ‹è¯•1: å•ä¸ªURL
        print("\nğŸ“ æµ‹è¯•1: å•ä¸ªURLæŠ“å–")
        test_urls = ["https://github.com/Neutrino1998/artifact-flow"]
        
        result = await tool(urls=test_urls)
        
        if result.success:
            print(f"âœ… æŠ“å–æˆåŠŸ")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print("\nXMLç»“æœï¼ˆå‰2000å­—ç¬¦ï¼‰:")
            print(result.data[:2000] + "...")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")
        
        # æµ‹è¯•2: å¤šä¸ªURL
        print("\nğŸ“ æµ‹è¯•2: æ‰¹é‡URLæŠ“å–ï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰")
        test_urls = [
            "https://github.com/Neutrino1998/artifact-flow",
            "https://www.python.org",
            "https://github.com"
        ]
        
        result = await tool(
            urls=test_urls,
            max_content_length=2000,
            max_concurrent=2  # é™åˆ¶å¹¶å‘æ•°é˜²æ­¢å†…å­˜é—®é¢˜
        )
        
        if result.success:
            print(f"âœ… æ‰¹é‡æŠ“å–å®Œæˆ (å¹¶å‘æ•°: 2)")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print(f"   å¤±è´¥: {result.metadata['failed_count']}")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test())
"""
Webå†…å®¹æŠ“å–å·¥å…·
åŸºäºcrawl4aiå®ç°ç½‘é¡µå†…å®¹çš„æ·±åº¦æŠ“å–ï¼Œæ”¯æŒHTMLå’ŒPDFæ–‡ä»¶
"""

import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from datetime import datetime
from io import BytesIO

from tools.base import BaseTool, ToolResult, ToolParameter, ToolPermission
from utils.logger import get_logger
import random

# å¯¼å…¥crawl4aiç»„ä»¶
try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    from crawl4ai.content_filter_strategy import PruningContentFilter
    from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
    from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher, RateLimiter
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False

# å¯¼å…¥PDFå¤„ç†
try:
    from pypdf import PdfReader
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

logger = get_logger("ArtifactFlow")


class WebFetchTool(BaseTool):
    """
    Webå†…å®¹æŠ“å–å·¥å…·
    ä½¿ç”¨crawl4aiæ·±åº¦æŠ“å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºç»“æ„åŒ–æ ¼å¼
    æ”¯æŒHTMLå’ŒPDFæ–‡ä»¶çš„æ™ºèƒ½æ£€æµ‹å’Œå¤„ç†
    
    ç‰¹æ€§ï¼š
    - æ™ºèƒ½ç±»å‹æ£€æµ‹ï¼šè‡ªåŠ¨è¯†åˆ«HTML/PDF/å…¶ä»–æ–‡ä»¶ç±»å‹
    - HTMLæŠ“å–ï¼šä½¿ç”¨crawl4aiè¿›è¡Œæ·±åº¦å†…å®¹æå–å’Œæ¸…æ´—
    - PDFå¤„ç†ï¼šä½¿ç”¨pypdfæå–PDFæ–‡æœ¬å†…å®¹
    - å†…å­˜è‡ªé€‚åº”ï¼šé€šè¿‡MemoryAdaptiveDispatcheræ§åˆ¶å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°
    - é˜²æ­¢å†…å­˜çˆ†ç‚¸ï¼šæ¯ä¸ªHTMLé¡µé¢ä¼šå¯åŠ¨ä¸€ä¸ªæµè§ˆå™¨å®ä¾‹ï¼Œä¸¥æ ¼æ§åˆ¶å¹¶å‘ä¿æŠ¤æœåŠ¡å™¨
    """
    
    def __init__(self):
        super().__init__(
            name="web_fetch",
            description="Fetch and extract content from web pages and PDF files",
            permission=ToolPermission.AUTO
        )
        
        if not CRAWL4AI_AVAILABLE:
            logger.error("crawl4ai is not available")
            return
        
        if not PDF_SUPPORT:
            logger.warning("pypdf is not installed. PDF support disabled. Install with: pip install pypdf")
        
        # User-Agent æ± ï¼ˆæ‰©å±•ç‰ˆï¼‰
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
        ]
        
        # åˆå§‹åŒ–æµè§ˆå™¨é…ç½®ï¼ˆç”¨äºHTMLï¼‰
        self.browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            user_agent=random.choice(self.user_agents)  # éšæœº User-Agent
        )
        
        # å†…å®¹è¿‡æ»¤å™¨é…ç½®
        self.prune_filter = PruningContentFilter(
            threshold=0.45,              # é€‚ä¸­çš„é˜ˆå€¼ï¼Œå¹³è¡¡å†…å®¹è´¨é‡å’Œæ•°é‡
            threshold_type="dynamic",   # åŠ¨æ€è°ƒæ•´é˜ˆå€¼
            # æ³¨æ„ï¼šä¸è®¾ç½®min_word_thresholdï¼Œé¿å…è¿‡æ»¤æ‰ä¸»ä½“å†…å®¹
        )
        
        # Markdownç”Ÿæˆå™¨é…ç½®
        self.md_generator = DefaultMarkdownGenerator(
            options={
                "ignore_links": True,      # ç§»é™¤è¶…é“¾æ¥
                "ignore_images": True,      # ç§»é™¤å›¾ç‰‡
                "escape_html": True,        # è½¬ä¹‰HTMLå®ä½“
                "skip_internal_links": True # è·³è¿‡å†…éƒ¨é“¾æ¥
            },
            content_filter=self.prune_filter
        )
        
        # è¿è¡Œé…ç½®
        self.run_config = CrawlerRunConfig(
            # å†…å®¹è¿‡æ»¤
            word_count_threshold=100,  # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå†…å®¹
            excluded_tags=['form', 'header', 'footer', 'nav'],  
            exclude_external_links=True,
            # å†…å®¹å¤„ç†
            process_iframes=True,
            remove_overlay_elements=True,
            # ç¼“å­˜æ§åˆ¶
            cache_mode=CacheMode.DISABLED,  # ç¦ç”¨ç¼“å­˜ï¼Œä¿è¯è·å–æœ€æ–°å†…å®¹
            # Markdownç”Ÿæˆå™¨
            markdown_generator=self.md_generator,
            # ç¦ç”¨æ—¥å¿—
            verbose=False
        )
    
    def get_parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="url_list",
                type="array[string]",
                description="URL or list of URLs to fetch (supports HTML and PDF)",
                required=True
            ),
            ToolParameter(
                name="max_content_length",
                type="integer",
                description="Maximum content length per page in characters (default: 10000)",
                required=False,
                default=10000
            ),
            ToolParameter(
                name="max_concurrent",
                type="integer",
                description="Maximum concurrent browser instances (default: 3, max: 5) - Each browser uses ~100-300MB memory",
                required=False,
                default=3
            )
        ]
    
    async def execute(self, **params) -> ToolResult:
        """
        æ‰§è¡Œç½‘é¡µæŠ“å–

        Args:
            url_list: URLå­—ç¬¦ä¸²æˆ–URLåˆ—è¡¨
            max_content_length: æ¯é¡µæœ€å¤§å†…å®¹é•¿åº¦
            max_concurrent: æœ€å¤§å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°

        Returns:
            ToolResult: åŒ…å«XMLæ ¼å¼çš„æŠ“å–ç»“æœ
        """
        if not CRAWL4AI_AVAILABLE:
            return ToolResult(
                success=False,
                error="crawl4ai is not installed. Please install it first."
            )
        
        # å‚æ•°å¤„ç†
        urls_param = params.get("url_list")
        if not urls_param:
            return ToolResult(success=False, error="url_list parameter is required")
        
        # ç¡®ä¿urlsæ˜¯åˆ—è¡¨
        if isinstance(urls_param, str):
            urls = [urls_param]
        elif isinstance(urls_param, list):
            urls = urls_param
        else:
            return ToolResult(success=False, error="url_list must be string or list")
        
        max_content_length = params.get("max_content_length", 10000)
        max_concurrent = min(params.get("max_concurrent", 3), 5)  # é™åˆ¶æœ€å¤§5ä¸ª
        
        logger.info(f"Fetching {len(urls)} URL(s) with max {max_concurrent} concurrent browsers")
        
        try:
            # æ‰§è¡ŒæŠ“å–
            results = await self._fetch_urls(urls, max_content_length, max_concurrent)
            
            # æ ¼å¼åŒ–ä¸ºXML
            xml_result = self._format_results_to_xml(results)
            
            # ç»Ÿè®¡ä¿¡æ¯
            success_count = sum(1 for r in results if r.get("success"))
            
            logger.info(f"Fetch completed: {success_count}/{len(urls)} successful")
            
            return ToolResult(
                success=True,
                data=xml_result,
                metadata={
                    "total_urls": len(urls),
                    "success_count": success_count,
                    "failed_count": len(urls) - success_count
                }
            )
            
        except Exception as e:
            logger.exception(f"Fetch failed: {str(e)}")
            return ToolResult(success=False, error=f"Fetch failed: {str(e)}")
    
    async def _detect_content_type(self, url: str) -> str:
        """
        æ£€æµ‹URLçš„å†…å®¹ç±»å‹
        
        Args:
            url: ç›®æ ‡URL
            
        Returns:
            'pdf', 'html', æˆ– 'unknown'
        """
        # 1. å…ˆé€šè¿‡URLåç¼€å¿«é€Ÿåˆ¤æ–­
        url_lower = url.lower()
        if url_lower.endswith('.pdf'):
            return 'pdf'
        elif any(url_lower.endswith(ext) for ext in ['.html', '.htm', '.php', '.asp', '.jsp']):
            return 'html'
        
        # 2. å‘é€HEADè¯·æ±‚æ£€æŸ¥Content-Type
        try:
            timeout = aiohttp.ClientTimeout(total=10)
            async with aiohttp.ClientSession() as session:
                async with session.head(
                    url, 
                    timeout=timeout,
                    allow_redirects=True,
                    headers={'User-Agent': random.choice(self.user_agents)}
                ) as response:
                    content_type = response.headers.get('Content-Type', '').lower()
                    
                    if 'pdf' in content_type or 'application/pdf' in content_type:
                        return 'pdf'
                    elif any(t in content_type for t in ['html', 'text/html', 'text/plain']):
                        return 'html'
                    
        except Exception as e:
            logger.warning(f"HEAD request failed for {url}: {e}, assuming HTML")
        
        # 3. é»˜è®¤æŒ‰HTMLå¤„ç†
        return 'html'
    
    async def _fetch_pdf(self, url: str, max_content_length: int) -> Dict[str, Any]:
        """
        æŠ“å–å¹¶è§£æPDFæ–‡ä»¶
        
        Args:
            url: PDFæ–‡ä»¶URL
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
            
        Returns:
            æŠ“å–ç»“æœå­—å…¸
        """
        if not PDF_SUPPORT:
            return {
                "success": False,
                "url": url,
                "error": "PDF support not available. Install pypdf: pip install pypdf"
            }
        
        try:
            logger.info(f"Fetching PDF: {url}")
            
            timeout = aiohttp.ClientTimeout(total=60)  # PDFå¯èƒ½è¾ƒå¤§ï¼Œ60ç§’è¶…æ—¶
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    timeout=timeout,
                    headers={'User-Agent': random.choice(self.user_agents)}
                ) as response:
                    if response.status != 200:
                        return {
                            "success": False,
                            "url": url,
                            "error": f"HTTP {response.status}"
                        }
                    
                    # æ£€æŸ¥Content-Type
                    content_type = response.headers.get('Content-Type', '').lower()
                    if 'pdf' not in content_type:
                        logger.warning(f"Expected PDF but got {content_type}, trying anyway...")
                    
                    # è¯»å–PDFå†…å®¹
                    pdf_bytes = await response.read()
                    
                    # ä½¿ç”¨pypdfæå–æ–‡æœ¬
                    pdf_file = BytesIO(pdf_bytes)
                    pdf_reader = PdfReader(pdf_file)
                    
                    # æå–æ‰€æœ‰é¡µé¢æ–‡æœ¬
                    text_parts = []
                    for page_num, page in enumerate(pdf_reader.pages, 1):
                        try:
                            page_text = page.extract_text()
                            if page_text.strip():
                                text_parts.append(page_text)
                        except Exception as e:
                            logger.warning(f"Failed to extract page {page_num}: {e}")
                    
                    full_text = "\n\n".join(text_parts)
                    
                    # è·å–PDFå…ƒæ•°æ®
                    title = "PDF Document"
                    if pdf_reader.metadata:
                        title = pdf_reader.metadata.get('/Title', title)
                    
                    # é™åˆ¶é•¿åº¦
                    if len(full_text) > max_content_length:
                        full_text = full_text[:max_content_length] + "\n\n[Content truncated...]"
                    
                    logger.info(f"PDF extracted: {len(text_parts)} pages, {len(full_text)} chars")
                    
                    return {
                        "success": True,
                        "url": url,
                        "title": title,
                        "content": full_text,
                        "word_count": len(full_text.split()),
                        "fetched_at": datetime.now().isoformat(),
                        "source_type": "pdf",
                        "page_count": len(pdf_reader.pages)
                    }
                    
        except Exception as e:
            logger.exception(f"PDF fetch failed for {url}")
            return {
                "success": False,
                "url": url,
                "error": f"PDF extraction failed: {str(e)}"
            }
    
    async def _fetch_html_urls(
        self,
        urls: List[str],
        max_content_length: int,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨crawl4aiæŠ“å–HTMLé¡µé¢ï¼ˆä½¿ç”¨MemoryAdaptiveDispatcherä¿æŠ¤å†…å­˜ï¼‰
        
        Args:
            urls: HTML URLåˆ—è¡¨
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            æŠ“å–ç»“æœåˆ—è¡¨
        """
        # åˆ›å»ºå†…å­˜è‡ªé€‚åº”è°ƒåº¦å™¨ - é˜²æ­¢å†…å­˜çˆ†ç‚¸
        dispatcher = MemoryAdaptiveDispatcher(
            memory_threshold_percent=70.0,  # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡70%æ—¶æš‚åœ
            check_interval=1.0,  # æ¯ç§’æ£€æŸ¥ä¸€æ¬¡å†…å­˜
            max_session_permit=max_concurrent,  # æœ€å¤§å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°
            memory_wait_timeout=120.0,  # è¶…æ—¶120ç§’æŠ›å‡ºé”™è¯¯
            rate_limiter=RateLimiter(
                base_delay=(0.5, 1.0),  # åŸºç¡€å»¶è¿Ÿ0.5-1ç§’
                max_delay=10.0,  # æœ€å¤§å»¶è¿Ÿ10ç§’
                max_retries=2  # æœ€å¤šé‡è¯•2æ¬¡
            ),
        )
        
        logger.info(f"Using MemoryAdaptiveDispatcher: max {max_concurrent} concurrent sessions, memory threshold 70%")
        
        # ä½¿ç”¨crawl4aiçš„æ‰¹é‡æŠ“å– + dispatcher
        async with AsyncWebCrawler(config=self.browser_config) as crawler:
            # arun_many ä¼šè‡ªåŠ¨ä½¿ç”¨ dispatcher æ§åˆ¶å¹¶å‘
            crawl_results = await crawler.arun_many(
                urls=urls,
                config=self.run_config,
                dispatcher=dispatcher  # ä¼ å…¥å†…å­˜æ§åˆ¶å™¨
            )
            
            # å¤„ç†ç»“æœ
            results = []
            for i, result in enumerate(crawl_results):
                url = urls[i]
                
                if result.success:
                    # æå–å†…å®¹ - ä½¿ç”¨æ–°çš„ markdown å±æ€§
                    content = result.markdown.fit_markdown or result.markdown.raw_markdown or ""
                    
                    # é™åˆ¶é•¿åº¦
                    if len(content) > max_content_length:
                        content = content[:max_content_length] + "\n\n[Content truncated...]"
                    
                    results.append({
                        "success": True,
                        "url": url,
                        "title": result.metadata.get("title", "Untitled") if result.metadata else "Untitled",
                        "content": content,
                        "word_count": len(content.split()),
                        "fetched_at": datetime.now().isoformat(),
                        "source_type": "html"
                    })
                    
                    logger.debug(f"Successfully fetched {url}: {len(content)} chars")
                else:
                    results.append({
                        "success": False,
                        "url": url,
                        "error": f"Crawl failed: {result.error_message or 'Unknown error'}"
                    })
                    
                    logger.warning(f"Failed to fetch {url}: {result.error_message}")
        
        return results
    
    async def _fetch_urls(
        self,
        urls: List[str],
        max_content_length: int,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æŠ“å–å¤šä¸ªURLï¼ˆæ™ºèƒ½æ£€æµ‹ç±»å‹å¹¶åˆ†åˆ«å¤„ç†ï¼‰
        
        Args:
            urls: URLåˆ—è¡¨
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
            max_concurrent: æœ€å¤§å¹¶å‘æµè§ˆå™¨å®ä¾‹æ•°
            
        Returns:
            æŠ“å–ç»“æœåˆ—è¡¨
        """
        # æ­¥éª¤1: æ£€æµ‹æ‰€æœ‰URLçš„ç±»å‹
        logger.info("Detecting content types...")
        content_types = await asyncio.gather(*[
            self._detect_content_type(url) for url in urls
        ])
        
        # æ­¥éª¤2: æŒ‰ç±»å‹åˆ†ç±»URL
        pdf_urls = []
        html_urls = []
        
        for url, content_type in zip(urls, content_types):
            if content_type == 'pdf':
                pdf_urls.append(url)
                logger.info(f"Detected as PDF: {url}")
            else:
                html_urls.append(url)
                logger.info(f"Detected as HTML: {url}")
        
        results = []
        
        # æ­¥éª¤3: å¤„ç†PDFæ–‡ä»¶ï¼ˆå¹¶å‘ï¼‰
        if pdf_urls:
            logger.info(f"Fetching {len(pdf_urls)} PDF file(s)...")
            pdf_results = await asyncio.gather(*[
                self._fetch_pdf(url, max_content_length) for url in pdf_urls
            ])
            results.extend(pdf_results)
        
        # æ­¥éª¤4: å¤„ç†HTMLé¡µé¢ï¼ˆä½¿ç”¨crawl4ai + å†…å­˜è‡ªé€‚åº”è°ƒåº¦ï¼‰
        if html_urls:
            logger.info(f"Fetching {len(html_urls)} HTML page(s)...")
            html_results = await self._fetch_html_urls(html_urls, max_content_length, max_concurrent)
            results.extend(html_results)
        
        return results
    
    def _format_results_to_xml(self, results: List[Dict[str, Any]]) -> str:
        """
        å°†æŠ“å–ç»“æœæ ¼å¼åŒ–ä¸ºXML
        
        Args:
            results: æŠ“å–ç»“æœåˆ—è¡¨
            
        Returns:
            XMLæ ¼å¼å­—ç¬¦ä¸²
        """
        xml_parts = ["<fetch_results>"]
        
        for result in results:
            if result.get("success"):
                # æˆåŠŸçš„ç»“æœ
                xml_parts.append("  <fetch_result>")
                xml_parts.append(f"    <url>{self._escape_xml(result['url'])}</url>")
                xml_parts.append(f"    <title>{self._escape_xml(result.get('title', 'Untitled'))}</title>")
                xml_parts.append(f"    <source_type>{result.get('source_type', 'unknown')}</source_type>")
                
                # PDFç‰¹æœ‰å­—æ®µ
                if result.get('page_count'):
                    xml_parts.append(f"    <page_count>{result['page_count']}</page_count>")
                
                xml_parts.append(f"    <content>{self._escape_xml(result['content'])}</content>")
                xml_parts.append(f"    <word_count>{result['word_count']}</word_count>")
                xml_parts.append(f"    <fetched_at>{result['fetched_at']}</fetched_at>")
                xml_parts.append("  </fetch_result>")
            else:
                # å¤±è´¥çš„ç»“æœ
                xml_parts.append("  <fetch_error>")
                xml_parts.append(f"    <url>{self._escape_xml(result['url'])}</url>")
                xml_parts.append(f"    <error>{self._escape_xml(result.get('error', 'Unknown error'))}</error>")
                xml_parts.append("  </fetch_error>")
        
        xml_parts.append("</fetch_results>")
        
        return "\n".join(xml_parts)
    
    def _escape_xml(self, text: str) -> str:
        """
        è½¬ä¹‰XMLç‰¹æ®Šå­—ç¬¦
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            è½¬ä¹‰åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # XMLç‰¹æ®Šå­—ç¬¦è½¬ä¹‰
        replacements = {
            "&": "&amp;",
            "<": "&lt;",
            ">": "&gt;",
            '"': "&quot;",
            "'": "&apos;"
        }
        
        for char, escaped in replacements.items():
            text = text.replace(char, escaped)
        
        return text


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test():
        print("\nğŸ§ª WebæŠ“å–å·¥å…·æµ‹è¯•ï¼ˆæ”¯æŒPDFï¼‰")
        print("="*60)
        
        if not CRAWL4AI_AVAILABLE:
            print("âŒ crawl4aiæœªå®‰è£…")
            return
        
        tool = WebFetchTool()
        
        # æµ‹è¯•1: HTMLé¡µé¢
        print("\nğŸ“„ æµ‹è¯•1: HTMLé¡µé¢æŠ“å–")
        test_urls = ["https://github.com/Neutrino1998/artifact-flow"]
        
        result = await tool(url_list=test_urls)
        
        if result.success:
            print(f"âœ… HTMLæŠ“å–æˆåŠŸ")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print("\nXMLç»“æœï¼ˆå‰1000å­—ç¬¦ï¼‰:")
            print(result.data[:1000] + "...")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")
        
        # æµ‹è¯•2: PDFæ–‡ä»¶
        print("\nğŸ“‘ æµ‹è¯•2: PDFæ–‡ä»¶æŠ“å–")
        # ä½¿ç”¨ä¸€ä¸ªå…¬å¼€çš„PDFæµ‹è¯•
        pdf_urls = ["https://arxiv.org/pdf/1706.03762.pdf"]  # Attention is All You Needè®ºæ–‡
        
        result = await tool(url_list=pdf_urls, max_content_length=5000)
        
        if result.success:
            print(f"âœ… PDFæŠ“å–æˆåŠŸ")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print("\nXMLç»“æœï¼ˆå‰1000å­—ç¬¦ï¼‰:")
            print(result.data[:1000] + "...")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")
        
        # æµ‹è¯•3: æ··åˆæŠ“å–
        print("\nğŸ”€ æµ‹è¯•3: æ··åˆæŠ“å–ï¼ˆHTML + PDFï¼‰")
        mixed_urls = [
            "https://www.python.org",
            "https://arxiv.org/pdf/1706.03762.pdf"
        ]
        
        result = await tool(
            url_list=mixed_urls,
            max_content_length=3000,
            max_concurrent=2
        )
        
        if result.success:
            print(f"âœ… æ··åˆæŠ“å–å®Œæˆ")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print(f"   å¤±è´¥: {result.metadata['failed_count']}")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test())
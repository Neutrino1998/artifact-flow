"""
Webå†…å®¹æŠ“å–å·¥å…·
åŸºäºJina Reader APIå®ç°ç½‘é¡µå†…å®¹æŠ“å–ï¼Œæ”¯æŒHTMLå’ŒPDFæ–‡ä»¶
é™çº§è·¯å¾„ï¼šHTML â†’ BeautifulSoupçº¯æ–‡æœ¬æå–ï¼ŒPDF â†’ pypdfæ–‡æœ¬æå–
"""

import asyncio
import os
import re
import aiohttp
from typing import List, Dict, Any, Optional
from datetime import datetime
from io import BytesIO

from tools.base import BaseTool, ToolResult, ToolParameter, ToolPermission
from utils.logger import get_logger
import random

from bs4 import BeautifulSoup

# å¯¼å…¥PDFå¤„ç†
try:
    from pypdf import PdfReader
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False

logger = get_logger("ArtifactFlow")

# Jina Reader APIé…ç½®
JINA_API_KEY = os.getenv("JINA_API_KEY")
JINA_BASE_URL = "https://r.jina.ai"
JINA_RETRY_MAX = 2
JINA_RETRY_DELAY = 30  # 429é™é¢æ—¶ç­‰å¾…ç§’æ•°


class WebFetchTool(BaseTool):
    """
    Webå†…å®¹æŠ“å–å·¥å…·
    ä½¿ç”¨Jina Reader APIæŠ“å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæ ¼å¼
    æ”¯æŒHTMLå’ŒPDFæ–‡ä»¶çš„ç»Ÿä¸€å¤„ç†

    ç‰¹æ€§ï¼š
    - Jina Reader APIï¼šç»Ÿä¸€å¤„ç†HTMLå’ŒPDFï¼Œè¿”å›clean markdown
    - 429é‡è¯•ï¼šå‘½ä¸­é™é¢æ—¶è‡ªåŠ¨ç­‰å¾…é‡è¯•
    - æ™ºèƒ½é™çº§ï¼šJinaå¤±è´¥åæŒ‰ç±»å‹é™çº§ï¼ˆPDF â†’ pypdfï¼ŒHTML â†’ BeautifulSoupï¼‰
    - å¹¶å‘æ§åˆ¶ï¼šSemaphoreæ§åˆ¶æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    """

    def __init__(self):
        super().__init__(
            name="web_fetch",
            description="Fetch and extract content from web pages and PDF files",
            permission=ToolPermission.CONFIRM
        )

        if not PDF_SUPPORT:
            logger.warning("pypdf is not installed. PDF fallback disabled. Install with: pip install pypdf")

        # User-Agent æ± 
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"
        ]

    def get_parameters(self) -> List[ToolParameter]:
        return [
            ToolParameter(
                name="url_list",
                type="array[string]",
                description="URL or list of URLs to fetch (supports HTML and PDF)",
                required=True
            ),
            ToolParameter(
                name="max_content_length",
                type="integer",
                description="Maximum content length per page in characters (default: 20000)",
                required=False,
                default=20000
            ),
            ToolParameter(
                name="max_concurrent",
                type="integer",
                description="Maximum concurrent fetch requests (default: 3, max: 5)",
                required=False,
                default=3
            )
        ]

    async def execute(self, **params) -> ToolResult:
        """
        æ‰§è¡Œç½‘é¡µæŠ“å–

        Args:
            url_list: URLå­—ç¬¦ä¸²æˆ–URLåˆ—è¡¨
            max_content_length: æ¯é¡µæœ€å¤§å†…å®¹é•¿åº¦
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

        Returns:
            ToolResult: åŒ…å«XMLæ ¼å¼çš„æŠ“å–ç»“æœ
        """
        # å‚æ•°å¤„ç†
        urls_param = params.get("url_list")
        if not urls_param:
            return ToolResult(success=False, error="url_list parameter is required")

        # ç¡®ä¿urlsæ˜¯åˆ—è¡¨
        if isinstance(urls_param, str):
            urls = [urls_param]
        elif isinstance(urls_param, list):
            urls = urls_param
        else:
            return ToolResult(success=False, error="url_list must be string or list")

        # SSRF é˜²æŠ¤ï¼šä»…å…è®¸ http/https åè®®
        for url in urls:
            if not url.lower().startswith(("http://", "https://")):
                return ToolResult(
                    success=False,
                    error=f"Unsupported URL scheme: {url}. Only http:// and https:// are allowed."
                )

        # é»˜è®¤å€¼å·²ç”± _apply_defaults å¡«å……
        max_content_length = params["max_content_length"]
        max_concurrent = max(1, min(params["max_concurrent"], 5))  # é™åˆ¶1-5

        logger.info(f"Fetching {len(urls)} URL(s) with max {max_concurrent} concurrent requests")

        try:
            # æ‰§è¡ŒæŠ“å–
            results = await self._fetch_urls(urls, max_content_length, max_concurrent)

            # æ ¼å¼åŒ–ä¸ºXML
            xml_result = self._format_results_to_xml(results)

            # ç»Ÿè®¡ä¿¡æ¯
            success_count = sum(1 for r in results if r.get("success"))

            logger.info(f"Fetch completed: {success_count}/{len(urls)} successful")

            return ToolResult(
                success=True,
                data=xml_result,
                metadata={
                    "total_urls": len(urls),
                    "success_count": success_count,
                    "failed_count": len(urls) - success_count
                }
            )

        except Exception as e:
            logger.exception(f"Fetch failed: {str(e)}")
            return ToolResult(success=False, error=f"Fetch failed: {str(e)}")

    def _detect_content_type(self, url: str) -> str:
        """
        é€šè¿‡ URL åç¼€æ£€æµ‹å†…å®¹ç±»å‹

        Args:
            url: ç›®æ ‡URL

        Returns:
            'pdf' æˆ– 'html'
        """
        url_lower = url.lower().split('?')[0]  # å»æ‰æŸ¥è¯¢å‚æ•°
        if url_lower.endswith('.pdf'):
            return 'pdf'
        return 'html'

    async def _fetch_urls(
        self,
        urls: List[str],
        max_content_length: int,
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æŠ“å–å¤šä¸ªURLï¼ˆç»Ÿä¸€èµ°Jinaï¼Œå¤±è´¥åæŒ‰ç±»å‹é™çº§ï¼‰

        Args:
            urls: URLåˆ—è¡¨
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

        Returns:
            æŠ“å–ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def _limited_fetch(url: str) -> Dict[str, Any]:
            async with semaphore:
                return await self._fetch_single_url(url, max_content_length)

        results = await asyncio.gather(*[_limited_fetch(url) for url in urls])
        return list(results)

    async def _fetch_single_url(self, url: str, max_content_length: int) -> Dict[str, Any]:
        """
        æŠ“å–å•ä¸ªURLï¼šå…ˆè¯•Jina Reader APIï¼Œå¤±è´¥åæŒ‰ç±»å‹é™çº§

        Args:
            url: ç›®æ ‡URL
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦

        Returns:
            æŠ“å–ç»“æœå­—å…¸
        """
        # ä¸»è·¯å¾„ï¼šJina Reader API
        jina_result = await self._fetch_via_jina(url, max_content_length)
        if jina_result is not None:
            return jina_result

        # é™çº§è·¯å¾„ï¼šæŒ‰ç±»å‹åˆ†åˆ«å¤„ç†
        content_type = self._detect_content_type(url)
        if content_type == 'pdf':
            logger.info(f"Jina failed for PDF, falling back to pypdf: {url}")
            return await self._fetch_pdf(url, max_content_length)
        else:
            logger.info(f"Jina failed for HTML, falling back to BeautifulSoup: {url}")
            return await self._fetch_via_bs4(url, max_content_length)

    async def _fetch_via_jina(self, url: str, max_content_length: int) -> Optional[Dict[str, Any]]:
        """
        é€šè¿‡Jina Reader APIæŠ“å–URLå†…å®¹

        429æ—¶sleep(30)é‡è¯•ï¼Œæœ€å¤šé‡è¯•JINA_RETRY_MAXæ¬¡ã€‚
        è¿”å›Noneè¡¨ç¤ºå½»åº•å¤±è´¥ï¼Œéœ€èµ°é™çº§è·¯å¾„ã€‚

        Args:
            url: ç›®æ ‡URL
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦

        Returns:
            æŠ“å–ç»“æœå­—å…¸ï¼Œæˆ–Noneè¡¨ç¤ºå¤±è´¥
        """
        jina_url = f"{JINA_BASE_URL}/{url}"
        headers = {
            "Accept": "text/markdown",
            "User-Agent": random.choice(self.user_agents),
        }
        if JINA_API_KEY:
            headers["Authorization"] = f"Bearer {JINA_API_KEY}"

        timeout = aiohttp.ClientTimeout(total=60)

        for attempt in range(1 + JINA_RETRY_MAX):
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(jina_url, headers=headers, timeout=timeout) as response:
                        if response.status == 200:
                            content = await response.text()

                            # æå–æ ‡é¢˜ï¼ˆJinaè¿”å›çš„markdownç¬¬ä¸€è¡Œé€šå¸¸æ˜¯ Title: xxxï¼‰
                            title = "Untitled"
                            title_match = re.match(r'^Title:\s*(.+)$', content, re.MULTILINE)
                            if title_match:
                                title = title_match.group(1).strip()

                            # é™åˆ¶é•¿åº¦
                            if len(content) > max_content_length:
                                content = content[:max_content_length] + "\n\n[Content truncated...]"

                            source_type = self._detect_content_type(url)
                            logger.debug(f"Jina fetched {url}: {len(content)} chars")

                            return {
                                "success": True,
                                "url": url,
                                "title": title,
                                "content": content,
                                "word_count": len(content.split()),
                                "fetched_at": datetime.now().isoformat(),
                                "source_type": source_type,
                            }

                        elif response.status == 429:
                            if attempt < JINA_RETRY_MAX:
                                logger.warning(
                                    f"Jina 429 rate limit for {url}, "
                                    f"waiting {JINA_RETRY_DELAY}s (attempt {attempt + 1}/{JINA_RETRY_MAX})"
                                )
                                await asyncio.sleep(JINA_RETRY_DELAY)
                                continue
                            else:
                                logger.warning(f"Jina 429 exhausted retries for {url}")
                                return None

                        else:
                            logger.warning(f"Jina HTTP {response.status} for {url}")
                            return None

            except asyncio.TimeoutError:
                logger.warning(f"Jina timeout for {url} (attempt {attempt + 1})")
                if attempt < JINA_RETRY_MAX:
                    continue
                return None
            except Exception as e:
                logger.warning(f"Jina error for {url}: {e}")
                return None

        return None

    async def _fetch_via_bs4(self, url: str, max_content_length: int) -> Dict[str, Any]:
        """
        é™çº§è·¯å¾„ï¼šaiohttpä¸‹è½½HTML + BeautifulSoupæå–çº¯æ–‡æœ¬

        Args:
            url: ç›®æ ‡URL
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦

        Returns:
            æŠ“å–ç»“æœå­—å…¸
        """
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            headers = {"User-Agent": random.choice(self.user_agents)}

            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, timeout=timeout) as response:
                    if response.status != 200:
                        return {
                            "success": False,
                            "url": url,
                            "error": f"HTTP {response.status}"
                        }

                    html = await response.text()

            # BeautifulSoupè§£æ
            soup = BeautifulSoup(html, "html.parser")

            # æå–æ ‡é¢˜
            title = "Untitled"
            title_tag = soup.find("title")
            if title_tag and title_tag.string:
                title = title_tag.string.strip()

            # ç§»é™¤æ— ç”¨æ ‡ç­¾
            for tag in soup.find_all(["script", "style", "nav", "header", "footer", "form", "aside"]):
                tag.decompose()

            # æå–çº¯æ–‡æœ¬
            content = soup.get_text(separator="\n")

            # æ¸…ç†å¤šä½™ç©ºè¡Œ
            content = re.sub(r'\n{3,}', '\n\n', content).strip()

            # é™åˆ¶é•¿åº¦
            if len(content) > max_content_length:
                content = content[:max_content_length] + "\n\n[Content truncated...]"

            logger.debug(f"BS4 fetched {url}: {len(content)} chars")

            return {
                "success": True,
                "url": url,
                "title": title,
                "content": content,
                "word_count": len(content.split()),
                "fetched_at": datetime.now().isoformat(),
                "source_type": "html",
            }

        except Exception as e:
            logger.warning(f"BS4 fetch failed for {url}: {e}")
            return {
                "success": False,
                "url": url,
                "error": f"Fetch failed: {str(e)}"
            }

    async def _fetch_pdf(self, url: str, max_content_length: int) -> Dict[str, Any]:
        """
        é™çº§è·¯å¾„ï¼šæŠ“å–å¹¶è§£æPDFæ–‡ä»¶ï¼ˆpypdfï¼‰

        Args:
            url: PDFæ–‡ä»¶URL
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦

        Returns:
            æŠ“å–ç»“æœå­—å…¸
        """
        if not PDF_SUPPORT:
            return {
                "success": False,
                "url": url,
                "error": "PDF support not available. Install pypdf: pip install pypdf"
            }

        try:
            logger.info(f"Fetching PDF: {url}")

            timeout = aiohttp.ClientTimeout(total=60)  # PDFå¯èƒ½è¾ƒå¤§ï¼Œ60ç§’è¶…æ—¶
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    timeout=timeout,
                    headers={'User-Agent': random.choice(self.user_agents)}
                ) as response:
                    if response.status != 200:
                        return {
                            "success": False,
                            "url": url,
                            "error": f"HTTP {response.status}"
                        }

                    # æ£€æŸ¥Content-Type
                    content_type = response.headers.get('Content-Type', '').lower()
                    if 'pdf' not in content_type:
                        logger.warning(f"Expected PDF but got {content_type}, trying anyway...")

                    # è¯»å–PDFå†…å®¹
                    pdf_bytes = await response.read()

                    # ä½¿ç”¨pypdfæå–æ–‡æœ¬
                    pdf_file = BytesIO(pdf_bytes)
                    pdf_reader = PdfReader(pdf_file)

                    # æå–æ‰€æœ‰é¡µé¢æ–‡æœ¬
                    text_parts = []
                    for page_num, page in enumerate(pdf_reader.pages, 1):
                        try:
                            page_text = page.extract_text()
                            if page_text.strip():
                                text_parts.append(page_text)
                        except Exception as e:
                            logger.warning(f"Failed to extract page {page_num}: {e}")

                    full_text = "\n\n".join(text_parts)

                    # è·å–PDFå…ƒæ•°æ®
                    title = "PDF Document"
                    if pdf_reader.metadata:
                        title = pdf_reader.metadata.get('/Title', title)

                    # é™åˆ¶é•¿åº¦
                    if len(full_text) > max_content_length:
                        full_text = full_text[:max_content_length] + "\n\n[Content truncated...]"

                    logger.info(f"PDF extracted: {len(text_parts)} pages, {len(full_text)} chars")

                    return {
                        "success": True,
                        "url": url,
                        "title": title,
                        "content": full_text,
                        "word_count": len(full_text.split()),
                        "fetched_at": datetime.now().isoformat(),
                        "source_type": "pdf",
                        "page_count": len(pdf_reader.pages)
                    }

        except Exception as e:
            logger.exception(f"PDF fetch failed for {url}")
            return {
                "success": False,
                "url": url,
                "error": f"PDF extraction failed: {str(e)}"
            }

    def _format_results_to_xml(self, results: List[Dict[str, Any]]) -> str:
        """
        å°†æŠ“å–ç»“æœæ ¼å¼åŒ–ä¸ºXML

        Args:
            results: æŠ“å–ç»“æœåˆ—è¡¨

        Returns:
            XMLæ ¼å¼å­—ç¬¦ä¸²
        """
        xml_parts = ["<fetch_results>"]

        for result in results:
            if result.get("success"):
                # æˆåŠŸçš„ç»“æœ
                xml_parts.append("  <fetch_result>")
                xml_parts.append(f"    <url>{self._escape_xml(result['url'])}</url>")
                xml_parts.append(f"    <title>{self._escape_xml(result.get('title', 'Untitled'))}</title>")
                xml_parts.append(f"    <source_type>{result.get('source_type', 'unknown')}</source_type>")

                # PDFç‰¹æœ‰å­—æ®µ
                if result.get('page_count'):
                    xml_parts.append(f"    <page_count>{result['page_count']}</page_count>")

                xml_parts.append(f"    <content>{self._escape_xml(result['content'])}</content>")
                xml_parts.append(f"    <word_count>{result['word_count']}</word_count>")
                xml_parts.append(f"    <fetched_at>{result['fetched_at']}</fetched_at>")
                xml_parts.append("  </fetch_result>")
            else:
                # å¤±è´¥çš„ç»“æœ
                xml_parts.append("  <fetch_error>")
                xml_parts.append(f"    <url>{self._escape_xml(result['url'])}</url>")
                xml_parts.append(f"    <error>{self._escape_xml(result.get('error', 'Unknown error'))}</error>")
                xml_parts.append("  </fetch_error>")

        xml_parts.append("</fetch_results>")

        return "\n".join(xml_parts)

    def _escape_xml(self, text: str) -> str:
        """
        è½¬ä¹‰XMLç‰¹æ®Šå­—ç¬¦

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            è½¬ä¹‰åçš„æ–‡æœ¬
        """
        if not text:
            return ""

        # XMLç‰¹æ®Šå­—ç¬¦è½¬ä¹‰
        replacements = {
            "&": "&amp;",
            "<": "&lt;",
            ">": "&gt;",
            '"': "&quot;",
            "'": "&apos;"
        }

        for char, escaped in replacements.items():
            text = text.replace(char, escaped)

        return text


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test():
        print("\nğŸ§ª WebæŠ“å–å·¥å…·æµ‹è¯•ï¼ˆJina Reader APIï¼‰")
        print("="*60)

        tool = WebFetchTool()

        # æµ‹è¯•1: HTMLé¡µé¢
        print("\nğŸ“„ æµ‹è¯•1: HTMLé¡µé¢æŠ“å–")
        test_urls = ["https://github.com/Neutrino1998/artifact-flow"]

        result = await tool(url_list=test_urls)

        if result.success:
            print(f"âœ… HTMLæŠ“å–æˆåŠŸ")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print("\nXMLç»“æœï¼ˆå‰1000å­—ç¬¦ï¼‰:")
            print(result.data[:1000] + "...")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")

        # æµ‹è¯•2: PDFæ–‡ä»¶
        print("\nğŸ“‘ æµ‹è¯•2: PDFæ–‡ä»¶æŠ“å–")
        pdf_urls = ["https://arxiv.org/pdf/1706.03762.pdf"]

        result = await tool(url_list=pdf_urls, max_content_length=5000)

        if result.success:
            print(f"âœ… PDFæŠ“å–æˆåŠŸ")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print("\nXMLç»“æœï¼ˆå‰1000å­—ç¬¦ï¼‰:")
            print(result.data[:1000] + "...")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")

        # æµ‹è¯•3: æ··åˆæŠ“å–
        print("\nğŸ”€ æµ‹è¯•3: æ··åˆæŠ“å–ï¼ˆHTML + PDFï¼‰")
        mixed_urls = [
            "https://www.python.org",
            "https://arxiv.org/pdf/1706.03762.pdf"
        ]

        result = await tool(
            url_list=mixed_urls,
            max_content_length=3000,
            max_concurrent=2
        )

        if result.success:
            print(f"âœ… æ··åˆæŠ“å–å®Œæˆ")
            print(f"   æˆåŠŸ: {result.metadata['success_count']}/{result.metadata['total_urls']}")
            print(f"   å¤±è´¥: {result.metadata['failed_count']}")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥: {result.error}")

    # è¿è¡Œæµ‹è¯•
    asyncio.run(test())
